{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# edges in layer 1 are 361\n",
      "# edges in layer 2 are 181\n",
      "# edges in layer 3 are 198\n",
      "# edges are 740\n",
      "# nodes are 29\n",
      "[22 21 19 11 25  0 12 15  2  8  5 28]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<networkx.classes.digraph.DiGraph at 0x7f344a4a6ac8>,\n",
       "  <networkx.classes.digraph.DiGraph at 0x7f344a4a6b38>,\n",
       "  <networkx.classes.digraph.DiGraph at 0x7f344a4a6ba8>],\n",
       " array([[[ 1.30589645e+00,  1.30589645e+00,  1.30589645e+00],\n",
       "         [-7.21451938e-01, -7.21451938e-01, -7.21451938e-01],\n",
       "         [ 1.63448359e+00,  1.63448359e+00,  1.63448359e+00],\n",
       "         [ 4.02993004e-01,  4.02993004e-01,  4.02993004e-01],\n",
       "         [-2.51839561e-01, -2.51839561e-01, -2.51839561e-01]],\n",
       " \n",
       "        [[ 3.61209064e-02,  3.61209064e-02,  3.61209064e-02],\n",
       "         [ 1.45226963e+00,  1.45226963e+00,  1.45226963e+00],\n",
       "         [-1.00387374e+00, -1.00387374e+00, -1.00387374e+00],\n",
       "         [ 9.87601125e-02,  9.87601125e-02,  9.87601125e-02],\n",
       "         [-1.68861688e-01, -1.68861688e-01, -1.68861688e-01]],\n",
       " \n",
       "        [[-2.54913694e-01, -2.54913694e-01, -2.54913694e-01],\n",
       "         [ 1.85182592e+00,  1.85182592e+00,  1.85182592e+00],\n",
       "         [ 2.99382121e-01,  2.99382121e-01,  2.99382121e-01],\n",
       "         [ 2.33719228e+00,  2.33719228e+00,  2.33719228e+00],\n",
       "         [-2.06926700e+00, -2.06926700e+00, -2.06926700e+00]],\n",
       " \n",
       "        [[ 2.03158786e+00,  2.03158786e+00,  2.03158786e+00],\n",
       "         [-1.06725036e-02, -1.06725036e-02, -1.06725036e-02],\n",
       "         [ 2.34863156e+00,  2.34863156e+00,  2.34863156e+00],\n",
       "         [ 1.25986533e-01,  1.25986533e-01,  1.25986533e-01],\n",
       "         [ 1.13590968e+00,  1.13590968e+00,  1.13590968e+00]],\n",
       " \n",
       "        [[-1.34546103e+00, -1.34546103e+00, -1.34546103e+00],\n",
       "         [ 5.91908744e-01,  5.91908744e-01,  5.91908744e-01],\n",
       "         [-2.37388678e-04, -2.37388678e-04, -2.37388678e-04],\n",
       "         [ 9.44135670e-02,  9.44135670e-02,  9.44135670e-02],\n",
       "         [ 4.58036358e-01,  4.58036358e-01,  4.58036358e-01]],\n",
       " \n",
       "        [[-5.22051235e-01, -5.22051235e-01, -5.22051235e-01],\n",
       "         [ 2.76869484e-01,  2.76869484e-01,  2.76869484e-01],\n",
       "         [ 1.95261600e+00,  1.95261600e+00,  1.95261600e+00],\n",
       "         [-6.90907656e-01, -6.90907656e-01, -6.90907656e-01],\n",
       "         [-2.68357854e-01, -2.68357854e-01, -2.68357854e-01]],\n",
       " \n",
       "        [[ 4.39381288e-01,  4.39381288e-01,  4.39381288e-01],\n",
       "         [-4.40851217e-01, -4.40851217e-01, -4.40851217e-01],\n",
       "         [-1.27178328e-01, -1.27178328e-01, -1.27178328e-01],\n",
       "         [-1.41700672e+00, -1.41700672e+00, -1.41700672e+00],\n",
       "         [ 8.74766602e-01,  8.74766602e-01,  8.74766602e-01]],\n",
       " \n",
       "        [[-1.65067298e+00, -1.65067298e+00, -1.65067298e+00],\n",
       "         [ 1.25440575e-01,  1.25440575e-01,  1.25440575e-01],\n",
       "         [ 8.85227105e-01,  8.85227105e-01,  8.85227105e-01],\n",
       "         [-9.58256833e-01, -9.58256833e-01, -9.58256833e-01],\n",
       "         [ 1.61072481e+00,  1.61072481e+00,  1.61072481e+00]],\n",
       " \n",
       "        [[-1.04805488e+00, -1.04805488e+00, -1.04805488e+00],\n",
       "         [ 1.74495796e+00,  1.74495796e+00,  1.74495796e+00],\n",
       "         [ 4.70494142e-01,  4.70494142e-01,  4.70494142e-01],\n",
       "         [ 1.05087637e-01,  1.05087637e-01,  1.05087637e-01],\n",
       "         [ 6.26552515e-01,  6.26552515e-01,  6.26552515e-01]],\n",
       " \n",
       "        [[-4.05670669e-01, -4.05670669e-01, -4.05670669e-01],\n",
       "         [-5.49559929e-01, -5.49559929e-01, -5.49559929e-01],\n",
       "         [ 4.82045109e-01,  4.82045109e-01,  4.82045109e-01],\n",
       "         [ 1.30864736e+00,  1.30864736e+00,  1.30864736e+00],\n",
       "         [ 6.24231806e-01,  6.24231806e-01,  6.24231806e-01]],\n",
       " \n",
       "        [[ 2.93287370e-01,  2.93287370e-01,  2.93287370e-01],\n",
       "         [-1.34099494e-01, -1.34099494e-01, -1.34099494e-01],\n",
       "         [-8.22571377e-01, -8.22571377e-01, -8.22571377e-01],\n",
       "         [-3.43713870e-01, -3.43713870e-01, -3.43713870e-01],\n",
       "         [ 2.35718037e-01,  2.35718037e-01,  2.35718037e-01]],\n",
       " \n",
       "        [[ 3.38356604e+00,  3.38356604e+00,  3.38356604e+00],\n",
       "         [ 1.08880205e+00,  1.08880205e+00,  1.08880205e+00],\n",
       "         [ 4.27386817e-01,  4.27386817e-01,  4.27386817e-01],\n",
       "         [ 5.63240112e-01,  5.63240112e-01,  5.63240112e-01],\n",
       "         [-9.24192493e-01, -9.24192493e-01, -9.24192493e-01]],\n",
       " \n",
       "        [[-5.78964399e-01, -5.78964399e-01, -5.78964399e-01],\n",
       "         [ 2.28111309e-01,  2.28111309e-01,  2.28111309e-01],\n",
       "         [-4.12941281e-01, -4.12941281e-01, -4.12941281e-01],\n",
       "         [-2.52742614e-01, -2.52742614e-01, -2.52742614e-01],\n",
       "         [-7.42449394e-01, -7.42449394e-01, -7.42449394e-01]],\n",
       " \n",
       "        [[-8.68613481e-01, -8.68613481e-01, -8.68613481e-01],\n",
       "         [-7.95988511e-01, -7.95988511e-01, -7.95988511e-01],\n",
       "         [ 3.08955246e-01,  3.08955246e-01,  3.08955246e-01],\n",
       "         [-4.83607932e-01, -4.83607932e-01, -4.83607932e-01],\n",
       "         [ 1.36460248e+00,  1.36460248e+00,  1.36460248e+00]],\n",
       " \n",
       "        [[ 1.78214619e+00,  1.78214619e+00,  1.78214619e+00],\n",
       "         [ 1.07843171e+00,  1.07843171e+00,  1.07843171e+00],\n",
       "         [-2.73417078e-01, -2.73417078e-01, -2.73417078e-01],\n",
       "         [ 3.74497446e-01,  3.74497446e-01,  3.74497446e-01],\n",
       "         [-1.15358924e-01, -1.15358924e-01, -1.15358924e-01]],\n",
       " \n",
       "        [[ 4.26439049e-01,  4.26439049e-01,  4.26439049e-01],\n",
       "         [-2.94670225e-01, -2.94670225e-01, -2.94670225e-01],\n",
       "         [-9.73692122e-01, -9.73692122e-01, -9.73692122e-01],\n",
       "         [ 8.67640099e-02,  8.67640099e-02,  8.67640099e-02],\n",
       "         [ 2.92252250e-01,  2.92252250e-01,  2.92252250e-01]],\n",
       " \n",
       "        [[ 1.57971721e+00,  1.57971721e+00,  1.57971721e+00],\n",
       "         [-1.74437541e+00, -1.74437541e+00, -1.74437541e+00],\n",
       "         [ 1.51819579e-01,  1.51819579e-01,  1.51819579e-01],\n",
       "         [ 1.44721779e+00,  1.44721779e+00,  1.44721779e+00],\n",
       "         [ 8.81433974e-02,  8.81433974e-02,  8.81433974e-02]],\n",
       " \n",
       "        [[ 6.29425670e-01,  6.29425670e-01,  6.29425670e-01],\n",
       "         [-5.31218017e-01, -5.31218017e-01, -5.31218017e-01],\n",
       "         [ 7.99188733e-01,  7.99188733e-01,  7.99188733e-01],\n",
       "         [-6.49878597e-01, -6.49878597e-01, -6.49878597e-01],\n",
       "         [ 1.41477657e+00,  1.41477657e+00,  1.41477657e+00]],\n",
       " \n",
       "        [[ 8.18258539e-01,  8.18258539e-01,  8.18258539e-01],\n",
       "         [ 2.79172939e-01,  2.79172939e-01,  2.79172939e-01],\n",
       "         [-5.45141156e-01, -5.45141156e-01, -5.45141156e-01],\n",
       "         [-6.16470678e-01, -6.16470678e-01, -6.16470678e-01],\n",
       "         [ 1.18279236e+00,  1.18279236e+00,  1.18279236e+00]],\n",
       " \n",
       "        [[ 1.48627109e+00,  1.48627109e+00,  1.48627109e+00],\n",
       "         [-4.24979481e-02, -4.24979481e-02, -4.24979481e-02],\n",
       "         [ 4.58823663e-01,  4.58823663e-01,  4.58823663e-01],\n",
       "         [ 1.04624322e+00,  1.04624322e+00,  1.04624322e+00],\n",
       "         [ 1.09381309e+00,  1.09381309e+00,  1.09381309e+00]],\n",
       " \n",
       "        [[ 1.84432317e-01,  1.84432317e-01,  1.84432317e-01],\n",
       "         [-2.82421024e-01, -2.82421024e-01, -2.82421024e-01],\n",
       "         [-5.20009479e-01, -5.20009479e-01, -5.20009479e-01],\n",
       "         [-4.37040658e-01, -4.37040658e-01, -4.37040658e-01],\n",
       "         [ 3.46771740e-01,  3.46771740e-01,  3.46771740e-01]],\n",
       " \n",
       "        [[-5.97374172e-03, -5.97374172e-03, -5.97374172e-03],\n",
       "         [-1.59554112e+00, -1.59554112e+00, -1.59554112e+00],\n",
       "         [-7.20955988e-02, -7.20955988e-02, -7.20955988e-02],\n",
       "         [-1.65656024e+00, -1.65656024e+00, -1.65656024e+00],\n",
       "         [ 4.89645675e-01,  4.89645675e-01,  4.89645675e-01]],\n",
       " \n",
       "        [[ 1.61450014e-01,  1.61450014e-01,  1.61450014e-01],\n",
       "         [-5.05372653e-01, -5.05372653e-01, -5.05372653e-01],\n",
       "         [-2.16380267e+00, -2.16380267e+00, -2.16380267e+00],\n",
       "         [-1.55815329e+00, -1.55815329e+00, -1.55815329e+00],\n",
       "         [-8.58851697e-01, -8.58851697e-01, -8.58851697e-01]],\n",
       " \n",
       "        [[ 4.03976024e-01,  4.03976024e-01,  4.03976024e-01],\n",
       "         [ 1.56128214e-01,  1.56128214e-01,  1.56128214e-01],\n",
       "         [ 4.55709044e-01,  4.55709044e-01,  4.55709044e-01],\n",
       "         [ 1.03130881e+00,  1.03130881e+00,  1.03130881e+00],\n",
       "         [-1.35274163e+00, -1.35274163e+00, -1.35274163e+00]],\n",
       " \n",
       "        [[-3.44212237e-01, -3.44212237e-01, -3.44212237e-01],\n",
       "         [-2.26891925e+00, -2.26891925e+00, -2.26891925e+00],\n",
       "         [ 1.52051297e+00,  1.52051297e+00,  1.52051297e+00],\n",
       "         [-3.56740409e-01, -3.56740409e-01, -3.56740409e-01],\n",
       "         [ 6.85913580e-01,  6.85913580e-01,  6.85913580e-01]],\n",
       " \n",
       "        [[-3.94306690e-01, -3.94306690e-01, -3.94306690e-01],\n",
       "         [ 5.56766363e-01,  5.56766363e-01,  5.56766363e-01],\n",
       "         [-1.25122505e+00, -1.25122505e+00, -1.25122505e+00],\n",
       "         [-2.40683260e-01, -2.40683260e-01, -2.40683260e-01],\n",
       "         [-7.91060717e-01, -7.91060717e-01, -7.91060717e-01]],\n",
       " \n",
       "        [[ 1.70672123e+00,  1.70672123e+00,  1.70672123e+00],\n",
       "         [-1.69449168e+00, -1.69449168e+00, -1.69449168e+00],\n",
       "         [ 1.55588085e+00,  1.55588085e+00,  1.55588085e+00],\n",
       "         [-5.49089281e-01, -5.49089281e-01, -5.49089281e-01],\n",
       "         [ 5.59778008e-01,  5.59778008e-01,  5.59778008e-01]],\n",
       " \n",
       "        [[-1.71657196e+00, -1.71657196e+00, -1.71657196e+00],\n",
       "         [ 2.02588412e+00,  2.02588412e+00,  2.02588412e+00],\n",
       "         [ 6.36171188e-01,  6.36171188e-01,  6.36171188e-01],\n",
       "         [ 1.05900394e+00,  1.05900394e+00,  1.05900394e+00],\n",
       "         [-8.39626567e-01, -8.39626567e-01, -8.39626567e-01]],\n",
       " \n",
       "        [[ 4.10838580e-01,  4.10838580e-01,  4.10838580e-01],\n",
       "         [-5.42599508e-02, -5.42599508e-02, -5.42599508e-02],\n",
       "         [-6.43071621e-01, -6.43071621e-01, -6.43071621e-01],\n",
       "         [-2.13938850e+00, -2.13938850e+00, -2.13938850e+00],\n",
       "         [ 6.01620396e-01,  6.01620396e-01,  6.01620396e-01]]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]),\n",
       " tensor([False,  True, False,  True,  True, False,  True,  True, False,  True,\n",
       "          True, False, False,  True,  True, False,  True,  True,  True, False,\n",
       "          True, False, False,  True,  True, False,  True,  True, False]),\n",
       " tensor([ True, False,  True, False, False,  True, False, False,  True, False,\n",
       "         False,  True,  True, False, False,  True, False, False, False,  True,\n",
       "         False,  True,  True, False, False,  True, False, False,  True]),\n",
       " tensor([ True, False,  True, False, False,  True, False, False,  True, False,\n",
       "         False,  True,  True, False, False,  True, False, False, False,  True,\n",
       "         False,  True,  True, False, False,  True, False, False,  True]),\n",
       " array([[[0, 0, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[1, 1, 0],\n",
       "         [0, 0, 0],\n",
       "         [1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [1, 0, 1],\n",
       "         [1, 1, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 1],\n",
       "         [0, 0, 0],\n",
       "         [1, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 0],\n",
       "         [1, 0, 0],\n",
       "         [0, 0, 0]]]))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_train_test_mask(num_nodes):\n",
    "    n = num_nodes\n",
    "    train_mask = np.zeros(n,dtype = bool)\n",
    "    random_indices = np.random.permutation(range(n))\n",
    "    train_indices = random_indices[:int(0.6*n)]\n",
    "    train_mask[train_indices] = True\n",
    "    test_mask = np.zeros(n,dtype = bool)\n",
    "    test_indices = random_indices[int(0.6*n):]\n",
    "    print(test_indices)\n",
    "    test_mask[test_indices]= True\n",
    "    return train_mask, test_mask\n",
    "\n",
    "def get_vicker_chan_dataset(multiplex_folder_path, size_x = 5):\n",
    "    vicker_data_folder = os.path.join(multiplex_folder_path, \"Vickers-Chan Dataset\" , \"Dataset\")\n",
    "    edges_file_path = os.path.join(vicker_data_folder,\"Vickers-Chan-7thGraders_multiplex.edges\" )\n",
    "    edges_df = pd.read_csv(edges_file_path, sep = \" \", header = None,  names = [\"layerId\", \"src\", \"dst\", \"weight\"],dtype=int)\n",
    "    edges_df['src'] = edges_df['src'] - 1 # index IDs from 0\n",
    "    edges_df['dst'] = edges_df['dst'] - 1 # index IDs from 0\n",
    "    layers = [1, 2, 3]\n",
    "    graphs = []\n",
    "    adj_mats = []\n",
    "    sum_ = 0\n",
    "    for layer in layers : \n",
    "        df = edges_df[edges_df['layerId'] == layer]\n",
    "        G= nx.from_pandas_edgelist(df, source='src', target='dst',create_using = nx.DiGraph)\n",
    "        graphs.append(G)\n",
    "        adj_mat = nx.adjacency_matrix(G).todense()\n",
    "        \n",
    "        adj_mats.append(np.array(adj_mat,dtype=int))\n",
    "        \n",
    "        sum_ += adj_mat.sum()\n",
    "        print(\"# edges in layer {} are {}\".format( layer, adj_mat.sum()))\n",
    "    print(\"# edges are {}\".format( sum_))\n",
    "    \n",
    "    n = max(edges_df[\"src\"]) + 1\n",
    "    print(\"# nodes are {}\".format( n ))\n",
    "    train_mask, test_mask = generate_train_test_mask(n)\n",
    "    random_X = np.random.normal(size = [n, size_x])\n",
    "    final_random_X = np.stack([random_X]* len(layers),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    labels = np.zeros(n,dtype = int) \n",
    "    labels[12:] = 1 # 0 for boy from index 0 - 11 , 12 - 28 is for girl\n",
    "    return graphs, final_random_X , torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), adj\n",
    "\n",
    "get_vicker_chan_dataset(\"/home/keshav/courses/master_thesis/multiplex_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edges_for_index(df, index_this, layer_id, G, col_prefix = \"vote\"):\n",
    "    index_vote = df.iloc[index_this].loc[\"{}{}\".format(col_prefix, layer_id)]\n",
    "    if(index_vote == \"?\"):\n",
    "        print(index_vote)\n",
    "        return []\n",
    "        \n",
    "    other_votes = [(index_this, val ) for val in list((df.loc[df[\"{}{}\".format(col_prefix, layer_id)] == index_vote]).index)]\n",
    "    #print(other_votes)\n",
    "    G.add_edges_from(other_votes)\n",
    "    return other_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# edges in layer 0 are 236\n",
      "# edges in layer 1 are 195\n",
      "# edges in layer 2 are 171\n",
      "# edges in layer 3 are 177\n",
      "# edges in layer 4 are 212\n",
      "# edges in layer 5 are 272\n",
      "# edges in layer 6 are 182\n",
      "# edges in layer 7 are 178\n",
      "# edges in layer 8 are 206\n",
      "# edges in layer 9 are 216\n",
      "# edges in layer 10 are 21\n",
      "# edges in layer 11 are 171\n",
      "# edges in layer 12 are 209\n",
      "# edges in layer 13 are 248\n",
      "# edges in layer 14 are 233\n",
      "# edges in layer 15 are 269\n",
      "# edges are 3196\n",
      "# nodes are 435\n",
      "[378   0 400 428 289  13 172  86 102 189 281  87 210  81 201 303 223  61\n",
      " 418  12 105 337 358  58 135 317 226  32 366 424 168  51  20 376  65 390\n",
      " 360 233 197 205 421 241 397 120  97 332  66 191 106 293 110 187 245 291\n",
      "  91 145 152  27  29 346   6 175 257 365 217 344  92 348 151 407 372 137\n",
      " 323  63 305 185 313 148 211 422 331  18 158  43 309  84 335 426 423 213\n",
      "   1  16 364 125 188  74 165 232 385 265  33  82 236 382 208 302 420 133\n",
      " 329 256 321  21 359  30  53 254 139  57 288 215 206 399  76  60 324 255\n",
      " 192 178  77 109 312 128  95  88 147 111  23 392 221 388  89 264 108 417\n",
      " 278 183   8 310 100 171  75 282 352  28 174 166 299 409 199 195 311  17\n",
      " 121 154  24 162 212 296  78 414 142 113 140 268]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_congress_dataset(multiplex_folder_path, size_x = 5):\n",
    "    vicker_data_folder = os.path.join(multiplex_folder_path, \"Congress Dataset\" )\n",
    "    edges_file_path = os.path.join(vicker_data_folder,\"house-votes-84.data\")\n",
    "    layer_ids = list(range(0,16))\n",
    "    edges_df = pd.read_csv(edges_file_path, sep = \",\", header = None,  names = [\"layerId\"] + [\"vote{}\".format(i) for i in layer_ids])\n",
    "    edges_df['labels'] = 0\n",
    "    edges_df.loc[edges_df['layerId'] == \"republican\",'labels'] = 1 \n",
    "    ids = np.array(list(range(len(edges_df))))\n",
    "    graphs_list = []\n",
    "    adj_mats = []\n",
    "    sum_ = 0\n",
    "    for layer in layer_ids:\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(ids)\n",
    "        for i in ids:\n",
    "            add_edges_for_index(edges_df, i, layer, G)\n",
    "            break\n",
    "        adj_mat = nx.adjacency_matrix(G).todense()\n",
    "        graphs_list.append(G)\n",
    "        adj_mats.append(np.array(adj_mat,dtype=int))\n",
    "        \n",
    "        sum_ += adj_mat.sum()\n",
    "        print(\"# edges in layer {} are {}\".format( layer, adj_mat.sum()))\n",
    "    \n",
    "    print(\"# edges are {}\".format( sum_))\n",
    "    \n",
    "    n = len(edges_df)\n",
    "    print(\"# nodes are {}\".format( n ))\n",
    "    train_mask, test_mask = generate_train_test_mask(n)\n",
    "    random_X = np.random.normal(size = [n, size_x])\n",
    "    final_random_X = np.stack([random_X]* len(layer_ids),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    labels = np.array(list(edges_df['labels']))\n",
    "    return graphs_list, final_random_X , torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), adj\n",
    "    \n",
    "\n",
    "res = get_congress_dataset(\"/home/keshav/courses/master_thesis/multiplex_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# edges in layer 0 are 345\n",
      "# edges in layer 1 are 32\n",
      "# edges in layer 2 are 95\n",
      "# edges in layer 3 are 136\n",
      "# edges in layer 4 are 798\n",
      "# edges are 1406\n",
      "# nodes are 961\n",
      "[621 688 781 141 918 373 277 890 664 439 560 311   9 281 496 237 815 845\n",
      " 620  78  79 299  44 803 471 113  22 539 928 156 341 627  63 301 398 241\n",
      " 544 132 668 384 703 558 484 715 432 742 774 929 457 733 419 324 456 748\n",
      " 493 624 111 618 478 399 771 906 463 170 436 896  42 334 672 804 718 140\n",
      " 234 948 726 388 208 258 907 159 892 636 925 653 332 369 955 103  90 114\n",
      " 841 511 801 469  89 546 893 589  21 107 578 913 529  29  27 216 327 756\n",
      " 650 531   1 743 247 339 657 936 939 798 723 438 452 886 368 870 606 775\n",
      " 240 422 273 379 853 705 195 864 593 525 713 566 406 178 307  16 135 651\n",
      " 116 564 378 749 877  96 480 391 791  43 402  33 356 574 244 874 451 157\n",
      " 201 326 808 412 349 212 608  32 623 611 846 246 194  95 414 567 945 110\n",
      " 649 934 652 351 655 700 895 171 746 303 404 283 336 772 375 317 181 935\n",
      " 521 686  93 435 338 767 403 372 235 163 434 735 151 729 852 405 292  10\n",
      " 609 702 903 585 231 223 660 409 256 666 581 689 470 149  47 445 346  83\n",
      " 730 784 401 352 501 161 658 847 189 220 753 129 304 440 357 622 725 485\n",
      " 788 526 287 950 856 719 619 282 673 165 825 224 123 789 947 792 917 661\n",
      " 855 109 396 167 758 563 523 894 467 122 799 944 810 476 274 184 805 940\n",
      " 594 142  17  36  14 506  94 411 134 443 633 245 260 768 168 400 147  98\n",
      " 433 555 899 139 816 537 200 842 276 569 817 425 271 595 669 179 814  37\n",
      "   7 477  35 494 459 731 905 610 930 933 734 592 465 143 949 376 313  64\n",
      " 197 310 665 824 174 862 769 536 169 330 517 382 229 458 720 481 510 191\n",
      " 362 576 663 570 561 604 155 812  12 266 926 295   8 773 259 754 830 535\n",
      " 242 370 614 199 684 921 466]\n"
     ]
    }
   ],
   "source": [
    "def get_mammo_dataset(multiplex_folder_path, size_x = 5):\n",
    "    mammo_data_folder = os.path.join(multiplex_folder_path, \"Mammogram Dataset\" )\n",
    "    edges_file_path = os.path.join(mammo_data_folder,\"mammographic_masses.data\")\n",
    "    layer_ids = list(range(0,5))\n",
    "    layer_names= [\"layer{}\".format(i) for i in layer_ids]\n",
    "    edges_df = pd.read_csv(edges_file_path, sep = \",\", header = None, names =  layer_names + [\"labels\"]  )\n",
    "    \n",
    "    ids = np.array(list(range(len(edges_df))))\n",
    "    graphs_list = []\n",
    "    adj_mats = []\n",
    "    sum_ = 0\n",
    "    for layer in layer_ids:\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(ids)\n",
    "        for i in ids:\n",
    "            add_edges_for_index(edges_df, i, layer, G, col_prefix=\"layer\")\n",
    "            break\n",
    "        adj_mat = nx.adjacency_matrix(G).todense()\n",
    "        graphs_list.append(G)\n",
    "        adj_mats.append(np.array(adj_mat,dtype=int))\n",
    "        \n",
    "        sum_ += adj_mat.sum()\n",
    "        print(\"# edges in layer {} are {}\".format( layer, adj_mat.sum()))\n",
    "    \n",
    "    print(\"# edges are {}\".format( sum_))\n",
    "    \n",
    "    n = len(edges_df)\n",
    "    print(\"# nodes are {}\".format( n ))\n",
    "    train_mask, test_mask = generate_train_test_mask(n)\n",
    "    X = edges_df.iloc[ids].loc[:,layer_names].replace(\"?\", -1).to_numpy().astype(float)\n",
    "    X = preprocessing.scale(X)\n",
    "    #random_X = np.random.normal(size = [n, size_x])\n",
    "    #final_random_X = np.stack([random_X]* len(layer_ids),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    labels = np.array(list(edges_df.iloc[ids]['labels'])).astype(int)\n",
    "    return graphs_list, X , torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), adj\n",
    "    \n",
    "\n",
    "res = get_mammo_dataset(\"/home/keshav/courses/master_thesis/multiplex_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mammo_dataset(multiplex_folder_path, size_x = 5):\n",
    "    mammo_data_folder = os.path.join(multiplex_folder_path, \"Mammogram Dataset\" )\n",
    "    edges_file_path = os.path.join(mammo_data_folder,\"mammographic_masses.data\")\n",
    "    layer_ids = list(range(0,5))\n",
    "    layer_names= [\"layer{}\".format(i) for i in layer_ids]\n",
    "    edges_df = pd.read_csv(edges_file_path, sep = \",\", header = None, names =  layer_names + [\"labels\"]  )\n",
    "    \n",
    "    ids = np.array(list(range(len(edges_df))))\n",
    "    graphs_list = []\n",
    "    adj_mats = []\n",
    "    sum_ = 0\n",
    "    for layer in layer_ids:\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(ids)\n",
    "        for i in ids:\n",
    "            add_edges_for_index(edges_df, i, layer, G, col_prefix=\"layer\")\n",
    "            break\n",
    "        adj_mat = nx.adjacency_matrix(G).todense()\n",
    "        graphs_list.append(G)\n",
    "        adj_mats.append(np.array(adj_mat,dtype=int))\n",
    "        \n",
    "        sum_ += adj_mat.sum()\n",
    "        print(\"# edges in layer {} are {}\".format( layer, adj_mat.sum()))\n",
    "    \n",
    "    print(\"# edges are {}\".format( sum_))\n",
    "    \n",
    "    n = len(edges_df)\n",
    "    print(\"# nodes are {}\".format( n ))\n",
    "    train_mask, test_mask = generate_train_test_mask(n)\n",
    "    X = edges_df.iloc[ids].loc[:,layer_names].replace(\"?\", -1).to_numpy().astype(float)\n",
    "    X = preprocessing.scale(X)\n",
    "    #random_X = np.random.normal(size = [n, size_x])\n",
    "    #final_random_X = np.stack([random_X]* len(layer_ids),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    labels = np.array(list(edges_df.iloc[ids]['labels'])).astype(int)\n",
    "    return graphs_list, X , torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), adj\n",
    "    \n",
    "\n",
    "res = get_mammo_dataset(\"/home/keshav/courses/master_thesis/multiplex_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  labels  layer0  layer1  layer2  layer3\n",
      "0      B       1       1       1       1\n",
      "1      R       1       1       1       2\n",
      "2      R       1       1       1       3\n",
      "3      R       1       1       1       4\n",
      "4      R       1       1       1       5\n",
      "# edges in layer 0 are 125\n",
      "# edges in layer 1 are 125\n",
      "# edges in layer 2 are 125\n",
      "# edges in layer 3 are 125\n",
      "# edges are 500\n",
      "# nodes are 625\n",
      "[248 291 572  85 590 108 182 562 529 136 351 494 526 403 303 281 588 295\n",
      " 258 138 507  15 538 202 286 427  76  92 139 368 419 442 345 113 199 210\n",
      " 441 358 459 595 586 577 404 398 354 127 198 578 150 435  22 125  84 305\n",
      " 143 251 456 158 156 353 159 176   0 464 416 216 564 428 541 478 120 592\n",
      " 620 567 271 576 304  80 514 162 482 331 276  83 379 234 471  58 275 205\n",
      " 371 554 423 297 155  26  12 417  59 493 270 289 175 392 561 596 511 247\n",
      " 341 383 605 366 530 594 260   2 609 357 254 173 377 114 233 129 179  78\n",
      " 284 224 306 178 161 350  86 488  36 546 374 201 491 421 591 446 460 429\n",
      " 325 333 280 505 509 105 388 473 145 439  82 256 406  75 283  45 570 236\n",
      " 149  93 450 535 532 391 160 132 465 101 384 381  39 547  28  79 135 560\n",
      " 277 131 157 215 434 533 264 126 314  94 558 497  69 540 539 486 339 348\n",
      " 611 580  35 451 373 329 426 327  20 462  98 151  70 241 363 322 343 230\n",
      " 133 148 231 542 624  40 195 438 116 154 261 623 324  32 163 525  50 104\n",
      " 336 469 115 246 211 187 296 581 227 601 294 338 452 235  42 321]\n",
      "[[-1.41421356 -1.41421356 -1.41421356 -1.41421356]\n",
      " [-1.41421356 -1.41421356 -1.41421356 -0.70710678]\n",
      " [-1.41421356 -1.41421356 -1.41421356  0.        ]\n",
      " ...\n",
      " [ 1.41421356  1.41421356  1.41421356  0.        ]\n",
      " [ 1.41421356  1.41421356  1.41421356  0.70710678]\n",
      " [ 1.41421356  1.41421356  1.41421356  1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "def get_balance_dataset(multiplex_folder_path, size_x = 5):\n",
    "    mammo_data_folder = os.path.join(multiplex_folder_path, \"Balance-Scale Dataset\" )\n",
    "    edges_file_path = os.path.join(mammo_data_folder,\"balance-scale.data\")\n",
    "    layer_ids = list(range(0,4))\n",
    "    layer_names= [\"layer{}\".format(i) for i in layer_ids]\n",
    "    edges_df = pd.read_csv(edges_file_path, sep = \",\", header = None, names = [\"labels\"]+ layer_names   )\n",
    "    print(edges_df.head())\n",
    "    ids = np.array(list(range(len(edges_df))))\n",
    "    graphs_list = []\n",
    "    adj_mats = []\n",
    "    sum_ = 0\n",
    "    for layer in layer_ids:\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(ids)\n",
    "        for i in ids:\n",
    "            add_edges_for_index(edges_df, i, layer, G, col_prefix=\"layer\")\n",
    "            break\n",
    "        adj_mat = nx.adjacency_matrix(G).todense()\n",
    "        graphs_list.append(G)\n",
    "        adj_mats.append(np.array(adj_mat,dtype=int))\n",
    "        \n",
    "        sum_ += adj_mat.sum()\n",
    "        print(\"# edges in layer {} are {}\".format( layer, adj_mat.sum()))\n",
    "    \n",
    "    print(\"# edges are {}\".format( sum_))\n",
    "    \n",
    "    n = len(edges_df)\n",
    "    print(\"# nodes are {}\".format( n ))\n",
    "    train_mask, test_mask = generate_train_test_mask(n)\n",
    "    X = edges_df.iloc[ids].loc[:,layer_names].replace(\"?\", -1).to_numpy().astype(float)\n",
    "    X = preprocessing.scale(X)\n",
    "    #random_X = np.random.normal(size = [n, size_x])\n",
    "    #final_random_X = np.stack([random_X]* len(layer_ids),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    edges_df[\"labels_style\"] = edges_df[\"labels\"].astype('category')\n",
    "    labels = np.array(list(edges_df.iloc[ids]['labels_style'].cat.codes))\n",
    "    return graphs_list, X , torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), adj\n",
    "    \n",
    "\n",
    "res = get_balance_dataset(\"/home/keshav/courses/master_thesis/multiplex_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "# edges in layer 0 are 225\n",
      "# edges in layer 1 are 227\n",
      "# edges in layer 2 are 761\n",
      "# edges in layer 3 are 623\n",
      "# edges are 1836\n",
      "# nodes are 191\n",
      "[ 88 132 155  52  87 113 101 161  22  68  41  82  44 135  76  39  92 138\n",
      "  25 164  45  34  77 144   5 145 187 105  13  21 167 185 112 128  14 151\n",
      "  37 173 133 177 100  91  98 169  73  83  24  15   0 141 139  50 180 114\n",
      "  59  43   6 118 189  17  90 102  49 190 111 175 115  58   1 108 122 152\n",
      "  42  47  96 129  19]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<networkx.classes.digraph.DiGraph at 0x7f3444c07a58>,\n",
       "  <networkx.classes.digraph.DiGraph at 0x7f3444c07f98>,\n",
       "  <networkx.classes.digraph.DiGraph at 0x7f344a3cb9e8>,\n",
       "  <networkx.classes.digraph.DiGraph at 0x7f344a3cb4e0>],\n",
       " array([[[-1.13913014, -1.13913014, -1.13913014, -1.13913014],\n",
       "         [-0.19130125, -0.19130125, -0.19130125, -0.19130125],\n",
       "         [-0.84218474, -0.84218474, -0.84218474, -0.84218474],\n",
       "         [ 0.54362255,  0.54362255,  0.54362255,  0.54362255],\n",
       "         [ 1.94488969,  1.94488969,  1.94488969,  1.94488969]],\n",
       " \n",
       "        [[-1.29630258, -1.29630258, -1.29630258, -1.29630258],\n",
       "         [-0.04711979, -0.04711979, -0.04711979, -0.04711979],\n",
       "         [-0.74389284, -0.74389284, -0.74389284, -0.74389284],\n",
       "         [-1.72291031, -1.72291031, -1.72291031, -1.72291031],\n",
       "         [-2.23690371, -2.23690371, -2.23690371, -2.23690371]],\n",
       " \n",
       "        [[-1.20319467, -1.20319467, -1.20319467, -1.20319467],\n",
       "         [-1.34063564, -1.34063564, -1.34063564, -1.34063564],\n",
       "         [ 0.33908717,  0.33908717,  0.33908717,  0.33908717],\n",
       "         [-0.41628469, -0.41628469, -0.41628469, -0.41628469],\n",
       "         [ 0.46670859,  0.46670859,  0.46670859,  0.46670859]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-0.28778191, -0.28778191, -0.28778191, -0.28778191],\n",
       "         [-0.84864706, -0.84864706, -0.84864706, -0.84864706],\n",
       "         [ 1.89199836,  1.89199836,  1.89199836,  1.89199836],\n",
       "         [-0.05349561, -0.05349561, -0.05349561, -0.05349561],\n",
       "         [-2.91122086, -2.91122086, -2.91122086, -2.91122086]],\n",
       " \n",
       "        [[ 1.15327835,  1.15327835,  1.15327835,  1.15327835],\n",
       "         [-1.61500802, -1.61500802, -1.61500802, -1.61500802],\n",
       "         [-0.30994236, -0.30994236, -0.30994236, -0.30994236],\n",
       "         [ 1.56964285,  1.56964285,  1.56964285,  1.56964285],\n",
       "         [-0.97514287, -0.97514287, -0.97514287, -0.97514287]],\n",
       " \n",
       "        [[-1.24971388, -1.24971388, -1.24971388, -1.24971388],\n",
       "         [-1.23190047, -1.23190047, -1.23190047, -1.23190047],\n",
       "         [-1.52592021, -1.52592021, -1.52592021, -1.52592021],\n",
       "         [-0.01961607, -0.01961607, -0.01961607, -0.01961607],\n",
       "         [-0.88162009, -0.88162009, -0.88162009, -0.88162009]]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "         1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        dtype=torch.int32),\n",
       " tensor([False, False,  True,  True,  True, False, False,  True,  True,  True,\n",
       "          True,  True,  True, False, False, False,  True, False,  True, False,\n",
       "          True, False, False,  True, False, False,  True,  True,  True,  True,\n",
       "          True,  True,  True,  True, False,  True,  True, False,  True, False,\n",
       "          True, False, False, False, False, False,  True, False,  True, False,\n",
       "         False,  True, False,  True,  True,  True,  True,  True, False, False,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True, False,  True,\n",
       "          True,  True,  True, False,  True,  True, False, False,  True,  True,\n",
       "          True,  True, False, False,  True,  True,  True, False, False,  True,\n",
       "         False, False, False,  True,  True,  True, False,  True, False,  True,\n",
       "         False, False, False,  True,  True, False,  True,  True, False,  True,\n",
       "          True, False, False, False, False, False,  True,  True, False,  True,\n",
       "          True,  True, False,  True,  True,  True,  True,  True, False, False,\n",
       "          True,  True, False, False,  True, False,  True,  True, False, False,\n",
       "          True, False,  True,  True, False, False,  True,  True,  True,  True,\n",
       "          True, False, False,  True,  True, False,  True,  True,  True,  True,\n",
       "          True, False,  True,  True, False,  True,  True, False,  True, False,\n",
       "          True,  True,  True, False,  True, False,  True, False,  True,  True,\n",
       "         False,  True,  True,  True,  True, False,  True, False,  True, False,\n",
       "         False]),\n",
       " tensor([ True,  True, False, False, False,  True,  True, False, False, False,\n",
       "         False, False, False,  True,  True,  True, False,  True, False,  True,\n",
       "         False,  True,  True, False,  True,  True, False, False, False, False,\n",
       "         False, False, False, False,  True, False, False,  True, False,  True,\n",
       "         False,  True,  True,  True,  True,  True, False,  True, False,  True,\n",
       "          True, False,  True, False, False, False, False, False,  True,  True,\n",
       "         False, False, False, False, False, False, False, False,  True, False,\n",
       "         False, False, False,  True, False, False,  True,  True, False, False,\n",
       "         False, False,  True,  True, False, False, False,  True,  True, False,\n",
       "          True,  True,  True, False, False, False,  True, False,  True, False,\n",
       "          True,  True,  True, False, False,  True, False, False,  True, False,\n",
       "         False,  True,  True,  True,  True,  True, False, False,  True, False,\n",
       "         False, False,  True, False, False, False, False, False,  True,  True,\n",
       "         False, False,  True,  True, False,  True, False, False,  True,  True,\n",
       "         False,  True, False, False,  True,  True, False, False, False, False,\n",
       "         False,  True,  True, False, False,  True, False, False, False, False,\n",
       "         False,  True, False, False,  True, False, False,  True, False,  True,\n",
       "         False, False, False,  True, False,  True, False,  True, False, False,\n",
       "          True, False, False, False, False,  True, False,  True, False,  True,\n",
       "          True]),\n",
       " tensor([ True,  True, False, False, False,  True,  True, False, False, False,\n",
       "         False, False, False,  True,  True,  True, False,  True, False,  True,\n",
       "         False,  True,  True, False,  True,  True, False, False, False, False,\n",
       "         False, False, False, False,  True, False, False,  True, False,  True,\n",
       "         False,  True,  True,  True,  True,  True, False,  True, False,  True,\n",
       "          True, False,  True, False, False, False, False, False,  True,  True,\n",
       "         False, False, False, False, False, False, False, False,  True, False,\n",
       "         False, False, False,  True, False, False,  True,  True, False, False,\n",
       "         False, False,  True,  True, False, False, False,  True,  True, False,\n",
       "          True,  True,  True, False, False, False,  True, False,  True, False,\n",
       "          True,  True,  True, False, False,  True, False, False,  True, False,\n",
       "         False,  True,  True,  True,  True,  True, False, False,  True, False,\n",
       "         False, False,  True, False, False, False, False, False,  True,  True,\n",
       "         False, False,  True,  True, False,  True, False, False,  True,  True,\n",
       "         False,  True, False, False,  True,  True, False, False, False, False,\n",
       "         False,  True,  True, False, False,  True, False, False, False, False,\n",
       "         False,  True, False, False,  True, False, False,  True, False,  True,\n",
       "         False, False, False,  True, False,  True, False,  True, False, False,\n",
       "          True, False, False, False, False,  True, False,  True, False,  True,\n",
       "          True]),\n",
       " array([[[1, 1, 1, 1],\n",
       "         [1, 1, 0, 0],\n",
       "         [1, 1, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[1, 1, 0, 0],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[1, 1, 0, 0],\n",
       "         [1, 1, 0, 0],\n",
       "         [1, 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1, 1],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [1, 1, 1, 1],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [1, 1, 1, 1]]]))"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_leskovec_dataset(multiplex_folder_path, size_x = 5):\n",
    "    les_data_folder = os.path.join(multiplex_folder_path, \"Leskovec-Ng Dataset\" )\n",
    "    edges_file_path = os.path.join(les_data_folder,\"Leskovec-Ng.multilayer.edges\")\n",
    "    labels = np.loadtxt(os.path.join(les_data_folder,'Leskovec-Ng.multilayer.labels')).astype(np.int32)\n",
    "    \n",
    "    data = np.loadtxt(fname=edges_file_path).astype(np.int32)\n",
    "    layers = [0, 1, 2, 3]\n",
    "    graphs = []\n",
    "    adj_mats = []\n",
    "    sum_ = 0\n",
    "    edges_df = pd.read_csv(edges_file_path, sep = \" \", header = None,  names = [\"layerId\", \"src\", \"dst\"],dtype=int)\n",
    "    print(edges_df['src'].min())\n",
    "    \n",
    "    for layer in layers : \n",
    "        df = edges_df[edges_df['layerId'] == layer]\n",
    "        G= nx.from_pandas_edgelist(df, source='src', target='dst',create_using = nx.DiGraph)\n",
    "        graphs.append(G)\n",
    "        adj_mat = nx.adjacency_matrix(G).todense()\n",
    "        \n",
    "        adj_mats.append(np.array(adj_mat,dtype=int))\n",
    "        \n",
    "        sum_ += adj_mat.sum()\n",
    "        print(\"# edges in layer {} are {}\".format( layer, adj_mat.sum()))\n",
    "    print(\"# edges are {}\".format( sum_))\n",
    "    \n",
    "    n = max(edges_df[\"src\"].max(), edges_df[\"dst\"].max())  + 1\n",
    "    print(\"# nodes are {}\".format( n ))\n",
    "    train_mask, test_mask = generate_train_test_mask(n)\n",
    "    random_X = np.random.normal(size = [n, size_x])\n",
    "    final_random_X = np.stack([random_X]* len(layers),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    \n",
    "    \n",
    "    return graphs, final_random_X , torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), adj\n",
    "    \n",
    "\n",
    "get_les_dataset(\"/home/keshav/courses/master_thesis/multiplex_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
