{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import networkx as nx\n",
    "import os\n",
    "import numpy as np\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# edges in layer 1 are 361\n",
      "# edges in layer 2 are 181\n",
      "# edges in layer 3 are 198\n",
      "# edges are 740\n",
      "# nodes are 29\n",
      "[26 28  3 23  8 18 27 20  0 19 21  9]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<networkx.classes.digraph.DiGraph at 0x7f4180877c88>,\n",
       "  <networkx.classes.digraph.DiGraph at 0x7f4180877cf8>,\n",
       "  <networkx.classes.digraph.DiGraph at 0x7f4180877d30>],\n",
       " array([[[ 0.84463349,  0.84463349,  0.84463349],\n",
       "         [-0.16880075, -0.16880075, -0.16880075],\n",
       "         [ 0.3103442 ,  0.3103442 ,  0.3103442 ],\n",
       "         [ 1.26937487,  1.26937487,  1.26937487],\n",
       "         [ 0.14938443,  0.14938443,  0.14938443]],\n",
       " \n",
       "        [[-0.03802018, -0.03802018, -0.03802018],\n",
       "         [-0.15277118, -0.15277118, -0.15277118],\n",
       "         [-1.43360586, -1.43360586, -1.43360586],\n",
       "         [ 0.19639727,  0.19639727,  0.19639727],\n",
       "         [ 1.1196925 ,  1.1196925 ,  1.1196925 ]],\n",
       " \n",
       "        [[-1.13090704, -1.13090704, -1.13090704],\n",
       "         [ 0.41809181,  0.41809181,  0.41809181],\n",
       "         [ 0.55268843,  0.55268843,  0.55268843],\n",
       "         [-1.18695125, -1.18695125, -1.18695125],\n",
       "         [-0.50097347, -0.50097347, -0.50097347]],\n",
       " \n",
       "        [[-0.26768899, -0.26768899, -0.26768899],\n",
       "         [-0.36287807, -0.36287807, -0.36287807],\n",
       "         [-1.65098097, -1.65098097, -1.65098097],\n",
       "         [-0.45778473, -0.45778473, -0.45778473],\n",
       "         [-0.20487795, -0.20487795, -0.20487795]],\n",
       " \n",
       "        [[-0.85276806, -0.85276806, -0.85276806],\n",
       "         [-0.08773029, -0.08773029, -0.08773029],\n",
       "         [ 0.86700217,  0.86700217,  0.86700217],\n",
       "         [-1.64365175, -1.64365175, -1.64365175],\n",
       "         [-1.79406139, -1.79406139, -1.79406139]],\n",
       " \n",
       "        [[-0.81971986, -0.81971986, -0.81971986],\n",
       "         [-0.7473304 , -0.7473304 , -0.7473304 ],\n",
       "         [-0.13977784, -0.13977784, -0.13977784],\n",
       "         [-1.3314151 , -1.3314151 , -1.3314151 ],\n",
       "         [ 1.82719479,  1.82719479,  1.82719479]],\n",
       " \n",
       "        [[ 0.75554944,  0.75554944,  0.75554944],\n",
       "         [ 0.23305563,  0.23305563,  0.23305563],\n",
       "         [-0.93218251, -0.93218251, -0.93218251],\n",
       "         [ 0.63990668,  0.63990668,  0.63990668],\n",
       "         [-0.1907461 , -0.1907461 , -0.1907461 ]],\n",
       " \n",
       "        [[-0.48823782, -0.48823782, -0.48823782],\n",
       "         [ 0.31487397,  0.31487397,  0.31487397],\n",
       "         [-1.82230441, -1.82230441, -1.82230441],\n",
       "         [ 1.61617281,  1.61617281,  1.61617281],\n",
       "         [-1.11823902, -1.11823902, -1.11823902]],\n",
       " \n",
       "        [[-1.8861801 , -1.8861801 , -1.8861801 ],\n",
       "         [ 0.36531469,  0.36531469,  0.36531469],\n",
       "         [ 0.58546974,  0.58546974,  0.58546974],\n",
       "         [-0.63677268, -0.63677268, -0.63677268],\n",
       "         [ 1.37799873,  1.37799873,  1.37799873]],\n",
       " \n",
       "        [[-0.39814725, -0.39814725, -0.39814725],\n",
       "         [-0.63113518, -0.63113518, -0.63113518],\n",
       "         [-0.10938167, -0.10938167, -0.10938167],\n",
       "         [ 0.528597  ,  0.528597  ,  0.528597  ],\n",
       "         [-0.27619283, -0.27619283, -0.27619283]],\n",
       " \n",
       "        [[-2.03482235, -2.03482235, -2.03482235],\n",
       "         [ 0.54385629,  0.54385629,  0.54385629],\n",
       "         [ 2.55964173,  2.55964173,  2.55964173],\n",
       "         [ 0.09911295,  0.09911295,  0.09911295],\n",
       "         [-0.25800366, -0.25800366, -0.25800366]],\n",
       " \n",
       "        [[-1.32854009, -1.32854009, -1.32854009],\n",
       "         [ 0.63635442,  0.63635442,  0.63635442],\n",
       "         [ 0.32454795,  0.32454795,  0.32454795],\n",
       "         [ 1.49604108,  1.49604108,  1.49604108],\n",
       "         [-1.0165631 , -1.0165631 , -1.0165631 ]],\n",
       " \n",
       "        [[ 0.5521525 ,  0.5521525 ,  0.5521525 ],\n",
       "         [-1.74077141, -1.74077141, -1.74077141],\n",
       "         [ 0.27403635,  0.27403635,  0.27403635],\n",
       "         [-0.28838386, -0.28838386, -0.28838386],\n",
       "         [ 0.71484609,  0.71484609,  0.71484609]],\n",
       " \n",
       "        [[-0.89421448, -0.89421448, -0.89421448],\n",
       "         [-0.00602591, -0.00602591, -0.00602591],\n",
       "         [-0.4170347 , -0.4170347 , -0.4170347 ],\n",
       "         [-0.64682367, -0.64682367, -0.64682367],\n",
       "         [-1.25734664, -1.25734664, -1.25734664]],\n",
       " \n",
       "        [[ 0.0930417 ,  0.0930417 ,  0.0930417 ],\n",
       "         [-0.35529229, -0.35529229, -0.35529229],\n",
       "         [-1.34822447, -1.34822447, -1.34822447],\n",
       "         [-0.7715137 , -0.7715137 , -0.7715137 ],\n",
       "         [-1.32975445, -1.32975445, -1.32975445]],\n",
       " \n",
       "        [[ 0.92419433,  0.92419433,  0.92419433],\n",
       "         [-0.97632255, -0.97632255, -0.97632255],\n",
       "         [-0.061422  , -0.061422  , -0.061422  ],\n",
       "         [ 0.11331165,  0.11331165,  0.11331165],\n",
       "         [-0.73336156, -0.73336156, -0.73336156]],\n",
       " \n",
       "        [[-0.67723555, -0.67723555, -0.67723555],\n",
       "         [-0.12516301, -0.12516301, -0.12516301],\n",
       "         [-1.14524894, -1.14524894, -1.14524894],\n",
       "         [-0.548436  , -0.548436  , -0.548436  ],\n",
       "         [-0.63278012, -0.63278012, -0.63278012]],\n",
       " \n",
       "        [[ 0.67640058,  0.67640058,  0.67640058],\n",
       "         [ 0.85540069,  0.85540069,  0.85540069],\n",
       "         [-1.6079403 , -1.6079403 , -1.6079403 ],\n",
       "         [-2.15912725, -2.15912725, -2.15912725],\n",
       "         [ 0.59864013,  0.59864013,  0.59864013]],\n",
       " \n",
       "        [[-0.0142197 , -0.0142197 , -0.0142197 ],\n",
       "         [-0.02664306, -0.02664306, -0.02664306],\n",
       "         [-1.15437778, -1.15437778, -1.15437778],\n",
       "         [ 0.48082631,  0.48082631,  0.48082631],\n",
       "         [ 1.61749328,  1.61749328,  1.61749328]],\n",
       " \n",
       "        [[ 0.08168169,  0.08168169,  0.08168169],\n",
       "         [-0.25269429, -0.25269429, -0.25269429],\n",
       "         [-1.01593383, -1.01593383, -1.01593383],\n",
       "         [ 0.65339694,  0.65339694,  0.65339694],\n",
       "         [ 1.21584627,  1.21584627,  1.21584627]],\n",
       " \n",
       "        [[ 1.65311827,  1.65311827,  1.65311827],\n",
       "         [ 1.41491115,  1.41491115,  1.41491115],\n",
       "         [-0.29143547, -0.29143547, -0.29143547],\n",
       "         [-0.70480534, -0.70480534, -0.70480534],\n",
       "         [ 0.66306198,  0.66306198,  0.66306198]],\n",
       " \n",
       "        [[ 1.31067734,  1.31067734,  1.31067734],\n",
       "         [-1.35147752, -1.35147752, -1.35147752],\n",
       "         [ 1.17074374,  1.17074374,  1.17074374],\n",
       "         [ 1.21116218,  1.21116218,  1.21116218],\n",
       "         [ 1.41504428,  1.41504428,  1.41504428]],\n",
       " \n",
       "        [[ 1.33289747,  1.33289747,  1.33289747],\n",
       "         [-0.29685902, -0.29685902, -0.29685902],\n",
       "         [ 2.27821582,  2.27821582,  2.27821582],\n",
       "         [-1.82587226, -1.82587226, -1.82587226],\n",
       "         [ 1.17818473,  1.17818473,  1.17818473]],\n",
       " \n",
       "        [[-0.47752124, -0.47752124, -0.47752124],\n",
       "         [ 1.76757864,  1.76757864,  1.76757864],\n",
       "         [-1.42883176, -1.42883176, -1.42883176],\n",
       "         [ 1.05251776,  1.05251776,  1.05251776],\n",
       "         [ 0.58861288,  0.58861288,  0.58861288]],\n",
       " \n",
       "        [[ 0.03693287,  0.03693287,  0.03693287],\n",
       "         [ 0.7652297 ,  0.7652297 ,  0.7652297 ],\n",
       "         [ 2.37178512,  2.37178512,  2.37178512],\n",
       "         [ 1.46353244,  1.46353244,  1.46353244],\n",
       "         [-0.61398584, -0.61398584, -0.61398584]],\n",
       " \n",
       "        [[-1.14790391, -1.14790391, -1.14790391],\n",
       "         [-0.21098365, -0.21098365, -0.21098365],\n",
       "         [ 0.25124901,  0.25124901,  0.25124901],\n",
       "         [ 1.6722016 ,  1.6722016 ,  1.6722016 ],\n",
       "         [ 0.17748452,  0.17748452,  0.17748452]],\n",
       " \n",
       "        [[ 0.97550147,  0.97550147,  0.97550147],\n",
       "         [ 0.49663812,  0.49663812,  0.49663812],\n",
       "         [-1.82428084, -1.82428084, -1.82428084],\n",
       "         [-1.88377538, -1.88377538, -1.88377538],\n",
       "         [ 1.49151973,  1.49151973,  1.49151973]],\n",
       " \n",
       "        [[-1.10809527, -1.10809527, -1.10809527],\n",
       "         [ 0.32602922,  0.32602922,  0.32602922],\n",
       "         [ 0.73468765,  0.73468765,  0.73468765],\n",
       "         [ 0.47428144,  0.47428144,  0.47428144],\n",
       "         [ 1.04526764,  1.04526764,  1.04526764]],\n",
       " \n",
       "        [[ 0.73540974,  0.73540974,  0.73540974],\n",
       "         [ 0.93153661,  0.93153661,  0.93153661],\n",
       "         [ 0.93993916,  0.93993916,  0.93993916],\n",
       "         [ 1.52831661,  1.52831661,  1.52831661],\n",
       "         [ 0.8406833 ,  0.8406833 ,  0.8406833 ]]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]),\n",
       " tensor([False,  True,  True, False,  True,  True,  True,  True, False, False,\n",
       "          True,  True,  True,  True,  True,  True,  True,  True, False, False,\n",
       "         False, False,  True, False,  True,  True, False, False, False]),\n",
       " tensor([ True, False, False,  True, False, False, False, False,  True,  True,\n",
       "         False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True, False,  True, False, False,  True,  True,  True]),\n",
       " tensor([ True, False, False,  True, False, False, False, False,  True,  True,\n",
       "         False, False, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True, False,  True, False, False,  True,  True,  True]),\n",
       " array([[[0, 0, 0],\n",
       "         [1, 1, 1],\n",
       "         [1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[1, 1, 0],\n",
       "         [0, 0, 0],\n",
       "         [1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0]],\n",
       " \n",
       "        [[1, 1, 0],\n",
       "         [1, 1, 1],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         [1, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0],\n",
       "         [1, 0, 1],\n",
       "         [1, 1, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         ...,\n",
       "         [1, 0, 1],\n",
       "         [0, 0, 0],\n",
       "         [1, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0],\n",
       "         [0, 0, 0],\n",
       "         [1, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 0],\n",
       "         [1, 0, 0],\n",
       "         [0, 0, 0]]]))"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def generate_train_test_mask(num_nodes):\n",
    "    n = num_nodes\n",
    "    train_mask = np.zeros(n,dtype = bool)\n",
    "    random_indices = np.random.permutation(range(n))\n",
    "    train_indices = random_indices[:int(0.6*n)]\n",
    "    train_mask[train_indices] = True\n",
    "    test_mask = np.zeros(n,dtype = bool)\n",
    "    test_indices = random_indices[int(0.6*n):]\n",
    "    print(test_indices)\n",
    "    test_mask[test_indices]= True\n",
    "    return train_mask, test_mask\n",
    "\n",
    "def get_vicker_chan_dataset(multiplex_folder_path, size_x = 5):\n",
    "    vicker_data_folder = os.path.join(multiplex_folder_path, \"Vickers-Chan Dataset\" , \"Dataset\")\n",
    "    edges_file_path = os.path.join(vicker_data_folder,\"Vickers-Chan-7thGraders_multiplex.edges\" )\n",
    "    edges_df = pd.read_csv(edges_file_path, sep = \" \", header = None,  names = [\"layerId\", \"src\", \"dst\", \"weight\"],dtype=int)\n",
    "    edges_df['src'] = edges_df['src'] - 1 # index IDs from 0\n",
    "    edges_df['dst'] = edges_df['dst'] - 1 # index IDs from 0\n",
    "    layers = [1, 2, 3]\n",
    "    graphs = []\n",
    "    adj_mats = []\n",
    "    sum_ = 0\n",
    "    for layer in layers : \n",
    "        df = edges_df[edges_df['layerId'] == layer]\n",
    "        G= nx.from_pandas_edgelist(df, source='src', target='dst',create_using = nx.DiGraph)\n",
    "        graphs.append(G)\n",
    "        adj_mat = nx.adjacency_matrix(G).todense()\n",
    "        \n",
    "        adj_mats.append(np.array(adj_mat,dtype=int))\n",
    "        \n",
    "        sum_ += adj_mat.sum()\n",
    "        print(\"# edges in layer {} are {}\".format( layer, adj_mat.sum()))\n",
    "    print(\"# edges are {}\".format( sum_))\n",
    "    \n",
    "    n = max(edges_df[\"src\"]) + 1\n",
    "    print(\"# nodes are {}\".format( n ))\n",
    "    train_mask, test_mask = generate_train_test_mask(n)\n",
    "    random_X = np.random.normal(size = [n, size_x])\n",
    "    final_random_X = np.stack([random_X]* len(layers),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    labels = np.zeros(n,dtype = int) \n",
    "    labels[12:] = 1 # 0 for boy from index 0 - 11 , 12 - 28 is for girl\n",
    "    return graphs, final_random_X , torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), adj\n",
    "\n",
    "get_vicker_chan_dataset(\"/home/keshav/courses/master_thesis/multiplex_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_edges_for_index(df, index_this, layer_id, G, col_prefix = \"vote\"):\n",
    "    index_vote = df.iloc[index_this].loc[\"{}{}\".format(col_prefix, layer_id)]\n",
    "    if(index_vote == \"?\"):\n",
    "        print(index_vote)\n",
    "        return []\n",
    "        \n",
    "    other_votes = [(index_this, val ) for val in list((df.loc[df[\"{}{}\".format(col_prefix, layer_id)] == index_vote]).index)]\n",
    "    #print(other_votes)\n",
    "    G.add_edges_from(other_votes)\n",
    "    return other_votes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# edges in layer 0 are 236\n",
      "# edges in layer 1 are 195\n",
      "# edges in layer 2 are 171\n",
      "# edges in layer 3 are 177\n",
      "# edges in layer 4 are 212\n",
      "# edges in layer 5 are 272\n",
      "# edges in layer 6 are 182\n",
      "# edges in layer 7 are 178\n",
      "# edges in layer 8 are 206\n",
      "# edges in layer 9 are 216\n",
      "# edges in layer 10 are 21\n",
      "# edges in layer 11 are 171\n",
      "# edges in layer 12 are 209\n",
      "# edges in layer 13 are 248\n",
      "# edges in layer 14 are 233\n",
      "# edges in layer 15 are 269\n",
      "# edges are 3196\n",
      "# nodes are 435\n",
      "[378   0 400 428 289  13 172  86 102 189 281  87 210  81 201 303 223  61\n",
      " 418  12 105 337 358  58 135 317 226  32 366 424 168  51  20 376  65 390\n",
      " 360 233 197 205 421 241 397 120  97 332  66 191 106 293 110 187 245 291\n",
      "  91 145 152  27  29 346   6 175 257 365 217 344  92 348 151 407 372 137\n",
      " 323  63 305 185 313 148 211 422 331  18 158  43 309  84 335 426 423 213\n",
      "   1  16 364 125 188  74 165 232 385 265  33  82 236 382 208 302 420 133\n",
      " 329 256 321  21 359  30  53 254 139  57 288 215 206 399  76  60 324 255\n",
      " 192 178  77 109 312 128  95  88 147 111  23 392 221 388  89 264 108 417\n",
      " 278 183   8 310 100 171  75 282 352  28 174 166 299 409 199 195 311  17\n",
      " 121 154  24 162 212 296  78 414 142 113 140 268]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "def get_congress_dataset(multiplex_folder_path, size_x = 5):\n",
    "    vicker_data_folder = os.path.join(multiplex_folder_path, \"Congress Dataset\" )\n",
    "    edges_file_path = os.path.join(vicker_data_folder,\"house-votes-84.data\")\n",
    "    layer_ids = list(range(0,16))\n",
    "    edges_df = pd.read_csv(edges_file_path, sep = \",\", header = None,  names = [\"layerId\"] + [\"vote{}\".format(i) for i in layer_ids])\n",
    "    edges_df['labels'] = 0\n",
    "    edges_df.loc[edges_df['layerId'] == \"republican\",'labels'] = 1 \n",
    "    ids = np.array(list(range(len(edges_df))))\n",
    "    graphs_list = []\n",
    "    adj_mats = []\n",
    "    sum_ = 0\n",
    "    for layer in layer_ids:\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(ids)\n",
    "        for i in ids:\n",
    "            add_edges_for_index(edges_df, i, layer, G)\n",
    "            break\n",
    "        adj_mat = nx.adjacency_matrix(G).todense()\n",
    "        graphs_list.append(G)\n",
    "        adj_mats.append(np.array(adj_mat,dtype=int))\n",
    "        \n",
    "        sum_ += adj_mat.sum()\n",
    "        print(\"# edges in layer {} are {}\".format( layer, adj_mat.sum()))\n",
    "    \n",
    "    print(\"# edges are {}\".format( sum_))\n",
    "    \n",
    "    n = len(edges_df)\n",
    "    print(\"# nodes are {}\".format( n ))\n",
    "    train_mask, test_mask = generate_train_test_mask(n)\n",
    "    random_X = np.random.normal(size = [n, size_x])\n",
    "    final_random_X = np.stack([random_X]* len(layer_ids),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    labels = np.array(list(edges_df['labels']))\n",
    "    return graphs_list, final_random_X , torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), adj\n",
    "    \n",
    "\n",
    "res = get_congress_dataset(\"/home/keshav/courses/master_thesis/multiplex_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# edges in layer 0 are 345\n",
      "# edges in layer 1 are 32\n",
      "# edges in layer 2 are 95\n",
      "# edges in layer 3 are 136\n",
      "# edges in layer 4 are 798\n",
      "# edges are 1406\n",
      "# nodes are 961\n",
      "[621 688 781 141 918 373 277 890 664 439 560 311   9 281 496 237 815 845\n",
      " 620  78  79 299  44 803 471 113  22 539 928 156 341 627  63 301 398 241\n",
      " 544 132 668 384 703 558 484 715 432 742 774 929 457 733 419 324 456 748\n",
      " 493 624 111 618 478 399 771 906 463 170 436 896  42 334 672 804 718 140\n",
      " 234 948 726 388 208 258 907 159 892 636 925 653 332 369 955 103  90 114\n",
      " 841 511 801 469  89 546 893 589  21 107 578 913 529  29  27 216 327 756\n",
      " 650 531   1 743 247 339 657 936 939 798 723 438 452 886 368 870 606 775\n",
      " 240 422 273 379 853 705 195 864 593 525 713 566 406 178 307  16 135 651\n",
      " 116 564 378 749 877  96 480 391 791  43 402  33 356 574 244 874 451 157\n",
      " 201 326 808 412 349 212 608  32 623 611 846 246 194  95 414 567 945 110\n",
      " 649 934 652 351 655 700 895 171 746 303 404 283 336 772 375 317 181 935\n",
      " 521 686  93 435 338 767 403 372 235 163 434 735 151 729 852 405 292  10\n",
      " 609 702 903 585 231 223 660 409 256 666 581 689 470 149  47 445 346  83\n",
      " 730 784 401 352 501 161 658 847 189 220 753 129 304 440 357 622 725 485\n",
      " 788 526 287 950 856 719 619 282 673 165 825 224 123 789 947 792 917 661\n",
      " 855 109 396 167 758 563 523 894 467 122 799 944 810 476 274 184 805 940\n",
      " 594 142  17  36  14 506  94 411 134 443 633 245 260 768 168 400 147  98\n",
      " 433 555 899 139 816 537 200 842 276 569 817 425 271 595 669 179 814  37\n",
      "   7 477  35 494 459 731 905 610 930 933 734 592 465 143 949 376 313  64\n",
      " 197 310 665 824 174 862 769 536 169 330 517 382 229 458 720 481 510 191\n",
      " 362 576 663 570 561 604 155 812  12 266 926 295   8 773 259 754 830 535\n",
      " 242 370 614 199 684 921 466]\n"
     ]
    }
   ],
   "source": [
    "def get_mammo_dataset(multiplex_folder_path, size_x = 5):\n",
    "    mammo_data_folder = os.path.join(multiplex_folder_path, \"Mammogram Dataset\" )\n",
    "    edges_file_path = os.path.join(mammo_data_folder,\"mammographic_masses.data\")\n",
    "    layer_ids = list(range(0,5))\n",
    "    layer_names= [\"layer{}\".format(i) for i in layer_ids]\n",
    "    edges_df = pd.read_csv(edges_file_path, sep = \",\", header = None, names =  layer_names + [\"labels\"]  )\n",
    "    \n",
    "    ids = np.array(list(range(len(edges_df))))\n",
    "    graphs_list = []\n",
    "    adj_mats = []\n",
    "    sum_ = 0\n",
    "    for layer in layer_ids:\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(ids)\n",
    "        for i in ids:\n",
    "            add_edges_for_index(edges_df, i, layer, G, col_prefix=\"layer\")\n",
    "            break\n",
    "        adj_mat = nx.adjacency_matrix(G).todense()\n",
    "        graphs_list.append(G)\n",
    "        adj_mats.append(np.array(adj_mat,dtype=int))\n",
    "        \n",
    "        sum_ += adj_mat.sum()\n",
    "        print(\"# edges in layer {} are {}\".format( layer, adj_mat.sum()))\n",
    "    \n",
    "    print(\"# edges are {}\".format( sum_))\n",
    "    \n",
    "    n = len(edges_df)\n",
    "    print(\"# nodes are {}\".format( n ))\n",
    "    train_mask, test_mask = generate_train_test_mask(n)\n",
    "    X = edges_df.iloc[ids].loc[:,layer_names].replace(\"?\", -1).to_numpy().astype(float)\n",
    "    X = preprocessing.scale(X)\n",
    "    #random_X = np.random.normal(size = [n, size_x])\n",
    "    #final_random_X = np.stack([random_X]* len(layer_ids),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    labels = np.array(list(edges_df.iloc[ids]['labels'])).astype(int)\n",
    "    return graphs_list, X , torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), adj\n",
    "    \n",
    "\n",
    "res = get_mammo_dataset(\"/home/keshav/courses/master_thesis/multiplex_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mammo_dataset(multiplex_folder_path, size_x = 5):\n",
    "    mammo_data_folder = os.path.join(multiplex_folder_path, \"Mammogram Dataset\" )\n",
    "    edges_file_path = os.path.join(mammo_data_folder,\"mammographic_masses.data\")\n",
    "    layer_ids = list(range(0,5))\n",
    "    layer_names= [\"layer{}\".format(i) for i in layer_ids]\n",
    "    edges_df = pd.read_csv(edges_file_path, sep = \",\", header = None, names =  layer_names + [\"labels\"]  )\n",
    "    \n",
    "    ids = np.array(list(range(len(edges_df))))\n",
    "    graphs_list = []\n",
    "    adj_mats = []\n",
    "    sum_ = 0\n",
    "    for layer in layer_ids:\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(ids)\n",
    "        for i in ids:\n",
    "            add_edges_for_index(edges_df, i, layer, G, col_prefix=\"layer\")\n",
    "            break\n",
    "        adj_mat = nx.adjacency_matrix(G).todense()\n",
    "        graphs_list.append(G)\n",
    "        adj_mats.append(np.array(adj_mat,dtype=int))\n",
    "        \n",
    "        sum_ += adj_mat.sum()\n",
    "        print(\"# edges in layer {} are {}\".format( layer, adj_mat.sum()))\n",
    "    \n",
    "    print(\"# edges are {}\".format( sum_))\n",
    "    \n",
    "    n = len(edges_df)\n",
    "    print(\"# nodes are {}\".format( n ))\n",
    "    train_mask, test_mask = generate_train_test_mask(n)\n",
    "    X = edges_df.iloc[ids].loc[:,layer_names].replace(\"?\", -1).to_numpy().astype(float)\n",
    "    X = preprocessing.scale(X)\n",
    "    #random_X = np.random.normal(size = [n, size_x])\n",
    "    #final_random_X = np.stack([random_X]* len(layer_ids),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    labels = np.array(list(edges_df.iloc[ids]['labels'])).astype(int)\n",
    "    return graphs_list, X , torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), adj\n",
    "    \n",
    "\n",
    "res = get_mammo_dataset(\"/home/keshav/courses/master_thesis/multiplex_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  labels  layer0  layer1  layer2  layer3\n",
      "0      B       1       1       1       1\n",
      "1      R       1       1       1       2\n",
      "2      R       1       1       1       3\n",
      "3      R       1       1       1       4\n",
      "4      R       1       1       1       5\n",
      "# edges in layer 0 are 125\n",
      "# edges in layer 1 are 125\n",
      "# edges in layer 2 are 125\n",
      "# edges in layer 3 are 125\n",
      "# edges are 500\n",
      "# nodes are 625\n",
      "[248 291 572  85 590 108 182 562 529 136 351 494 526 403 303 281 588 295\n",
      " 258 138 507  15 538 202 286 427  76  92 139 368 419 442 345 113 199 210\n",
      " 441 358 459 595 586 577 404 398 354 127 198 578 150 435  22 125  84 305\n",
      " 143 251 456 158 156 353 159 176   0 464 416 216 564 428 541 478 120 592\n",
      " 620 567 271 576 304  80 514 162 482 331 276  83 379 234 471  58 275 205\n",
      " 371 554 423 297 155  26  12 417  59 493 270 289 175 392 561 596 511 247\n",
      " 341 383 605 366 530 594 260   2 609 357 254 173 377 114 233 129 179  78\n",
      " 284 224 306 178 161 350  86 488  36 546 374 201 491 421 591 446 460 429\n",
      " 325 333 280 505 509 105 388 473 145 439  82 256 406  75 283  45 570 236\n",
      " 149  93 450 535 532 391 160 132 465 101 384 381  39 547  28  79 135 560\n",
      " 277 131 157 215 434 533 264 126 314  94 558 497  69 540 539 486 339 348\n",
      " 611 580  35 451 373 329 426 327  20 462  98 151  70 241 363 322 343 230\n",
      " 133 148 231 542 624  40 195 438 116 154 261 623 324  32 163 525  50 104\n",
      " 336 469 115 246 211 187 296 581 227 601 294 338 452 235  42 321]\n",
      "[[-1.41421356 -1.41421356 -1.41421356 -1.41421356]\n",
      " [-1.41421356 -1.41421356 -1.41421356 -0.70710678]\n",
      " [-1.41421356 -1.41421356 -1.41421356  0.        ]\n",
      " ...\n",
      " [ 1.41421356  1.41421356  1.41421356  0.        ]\n",
      " [ 1.41421356  1.41421356  1.41421356  0.70710678]\n",
      " [ 1.41421356  1.41421356  1.41421356  1.41421356]]\n"
     ]
    }
   ],
   "source": [
    "def get_balance_dataset(multiplex_folder_path, size_x = 5):\n",
    "    mammo_data_folder = os.path.join(multiplex_folder_path, \"Balance-Scale Dataset\" )\n",
    "    edges_file_path = os.path.join(mammo_data_folder,\"balance-scale.data\")\n",
    "    layer_ids = list(range(0,4))\n",
    "    layer_names= [\"layer{}\".format(i) for i in layer_ids]\n",
    "    edges_df = pd.read_csv(edges_file_path, sep = \",\", header = None, names = [\"labels\"]+ layer_names   )\n",
    "    print(edges_df.head())\n",
    "    ids = np.array(list(range(len(edges_df))))\n",
    "    graphs_list = []\n",
    "    adj_mats = []\n",
    "    sum_ = 0\n",
    "    for layer in layer_ids:\n",
    "        G = nx.DiGraph()\n",
    "        G.add_nodes_from(ids)\n",
    "        for i in ids:\n",
    "            add_edges_for_index(edges_df, i, layer, G, col_prefix=\"layer\")\n",
    "            break\n",
    "        adj_mat = nx.adjacency_matrix(G).todense()\n",
    "        graphs_list.append(G)\n",
    "        adj_mats.append(np.array(adj_mat,dtype=int))\n",
    "        \n",
    "        sum_ += adj_mat.sum()\n",
    "        print(\"# edges in layer {} are {}\".format( layer, adj_mat.sum()))\n",
    "    \n",
    "    print(\"# edges are {}\".format( sum_))\n",
    "    \n",
    "    n = len(edges_df)\n",
    "    print(\"# nodes are {}\".format( n ))\n",
    "    train_mask, test_mask = generate_train_test_mask(n)\n",
    "    X = edges_df.iloc[ids].loc[:,layer_names].replace(\"?\", -1).to_numpy().astype(float)\n",
    "    X = preprocessing.scale(X)\n",
    "    #random_X = np.random.normal(size = [n, size_x])\n",
    "    #final_random_X = np.stack([random_X]* len(layer_ids),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    edges_df[\"labels_style\"] = edges_df[\"labels\"].astype('category')\n",
    "    labels = np.array(list(edges_df.iloc[ids]['labels_style'].cat.codes))\n",
    "    return graphs_list, X , torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), adj\n",
    "    \n",
    "\n",
    "res = get_balance_dataset(\"/home/keshav/courses/master_thesis/multiplex_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "# edges in layer 0 are 225\n",
      "# edges in layer 1 are 227\n",
      "# edges in layer 2 are 761\n",
      "# edges in layer 3 are 623\n",
      "# edges are 1836\n",
      "# nodes are 191\n",
      "[130  73 108 111   7   6  38 177  99 127  60 182  62 138 152  20 155  76\n",
      " 172  27   2 169  87  58 147  49  82 171  86 174 144 139  12 116  96   1\n",
      " 151  23  22 175 112 159 154  34  46 187 168  54  36  26 167  37  29 156\n",
      " 178 109   8 170 135 110  65  40 129  56  13 153  93 137  14  70 184 122\n",
      " 101 120  41 160  64]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([<networkx.classes.digraph.DiGraph at 0x7f3447b5d400>,\n",
       "  <networkx.classes.digraph.DiGraph at 0x7f3447b5d080>,\n",
       "  <networkx.classes.digraph.DiGraph at 0x7f3447b5d5f8>,\n",
       "  <networkx.classes.digraph.DiGraph at 0x7f3447b5d630>],\n",
       " array([[[-0.04222158, -0.04222158, -0.04222158, -0.04222158],\n",
       "         [ 0.25202653,  0.25202653,  0.25202653,  0.25202653],\n",
       "         [ 0.58025645,  0.58025645,  0.58025645,  0.58025645],\n",
       "         [ 0.74788162,  0.74788162,  0.74788162,  0.74788162],\n",
       "         [ 0.31653638,  0.31653638,  0.31653638,  0.31653638]],\n",
       " \n",
       "        [[ 1.31203142,  1.31203142,  1.31203142,  1.31203142],\n",
       "         [ 0.0081169 ,  0.0081169 ,  0.0081169 ,  0.0081169 ],\n",
       "         [ 0.98317348,  0.98317348,  0.98317348,  0.98317348],\n",
       "         [ 2.35307381,  2.35307381,  2.35307381,  2.35307381],\n",
       "         [-0.039751  , -0.039751  , -0.039751  , -0.039751  ]],\n",
       " \n",
       "        [[-1.15457347, -1.15457347, -1.15457347, -1.15457347],\n",
       "         [ 0.05305854,  0.05305854,  0.05305854,  0.05305854],\n",
       "         [ 1.63679496,  1.63679496,  1.63679496,  1.63679496],\n",
       "         [ 0.25232942,  0.25232942,  0.25232942,  0.25232942],\n",
       "         [-1.50866793, -1.50866793, -1.50866793, -1.50866793]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[-1.25305908, -1.25305908, -1.25305908, -1.25305908],\n",
       "         [-1.60203468, -1.60203468, -1.60203468, -1.60203468],\n",
       "         [ 0.20490716,  0.20490716,  0.20490716,  0.20490716],\n",
       "         [-0.18050778, -0.18050778, -0.18050778, -0.18050778],\n",
       "         [-2.2504479 , -2.2504479 , -2.2504479 , -2.2504479 ]],\n",
       " \n",
       "        [[ 1.65523109,  1.65523109,  1.65523109,  1.65523109],\n",
       "         [ 1.37119768,  1.37119768,  1.37119768,  1.37119768],\n",
       "         [-0.29299559, -0.29299559, -0.29299559, -0.29299559],\n",
       "         [-0.49542162, -0.49542162, -0.49542162, -0.49542162],\n",
       "         [ 1.49854387,  1.49854387,  1.49854387,  1.49854387]],\n",
       " \n",
       "        [[ 0.2357717 ,  0.2357717 ,  0.2357717 ,  0.2357717 ],\n",
       "         [ 0.52701086,  0.52701086,  0.52701086,  0.52701086],\n",
       "         [ 1.39224168,  1.39224168,  1.39224168,  1.39224168],\n",
       "         [-0.8540268 , -0.8540268 , -0.8540268 , -0.8540268 ],\n",
       "         [-0.14064582, -0.14064582, -0.14064582, -0.14064582]]]),\n",
       " tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "         0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1,\n",
       "         1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1,\n",
       "         1, 0, 0, 0, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
       "         0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1,\n",
       "         1, 1, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
       "        dtype=torch.int32),\n",
       " tensor([ True, False, False,  True,  True,  True, False, False, False,  True,\n",
       "          True,  True, False, False, False,  True,  True,  True,  True,  True,\n",
       "         False,  True, False, False,  True,  True, False, False,  True, False,\n",
       "          True,  True,  True,  True, False,  True, False, False, False,  True,\n",
       "         False, False,  True,  True,  True,  True, False,  True,  True, False,\n",
       "          True,  True,  True,  True, False,  True, False,  True, False,  True,\n",
       "         False,  True, False,  True, False, False,  True,  True,  True,  True,\n",
       "         False,  True,  True, False,  True,  True, False,  True,  True,  True,\n",
       "          True,  True, False,  True,  True,  True, False, False,  True,  True,\n",
       "          True,  True,  True, False,  True,  True, False,  True,  True, False,\n",
       "          True, False,  True,  True,  True,  True,  True,  True, False, False,\n",
       "         False, False, False,  True,  True,  True, False,  True,  True,  True,\n",
       "         False,  True, False,  True,  True,  True,  True, False,  True, False,\n",
       "         False,  True,  True,  True,  True, False,  True, False, False, False,\n",
       "          True,  True,  True,  True, False,  True,  True, False,  True,  True,\n",
       "          True, False, False, False, False, False, False,  True,  True, False,\n",
       "         False,  True,  True,  True,  True,  True,  True, False, False, False,\n",
       "         False, False, False,  True, False, False,  True, False, False,  True,\n",
       "          True,  True, False,  True, False,  True,  True, False,  True,  True,\n",
       "          True]),\n",
       " tensor([False,  True,  True, False, False, False,  True,  True,  True, False,\n",
       "         False, False,  True,  True,  True, False, False, False, False, False,\n",
       "          True, False,  True,  True, False, False,  True,  True, False,  True,\n",
       "         False, False, False, False,  True, False,  True,  True,  True, False,\n",
       "          True,  True, False, False, False, False,  True, False, False,  True,\n",
       "         False, False, False, False,  True, False,  True, False,  True, False,\n",
       "          True, False,  True, False,  True,  True, False, False, False, False,\n",
       "          True, False, False,  True, False, False,  True, False, False, False,\n",
       "         False, False,  True, False, False, False,  True,  True, False, False,\n",
       "         False, False, False,  True, False, False,  True, False, False,  True,\n",
       "         False,  True, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True,  True, False, False, False,  True, False, False, False,\n",
       "          True, False,  True, False, False, False, False,  True, False,  True,\n",
       "          True, False, False, False, False,  True, False,  True,  True,  True,\n",
       "         False, False, False, False,  True, False, False,  True, False, False,\n",
       "         False,  True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "          True, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True, False,  True,  True, False,  True,  True, False,\n",
       "         False, False,  True, False,  True, False, False,  True, False, False,\n",
       "         False]),\n",
       " tensor([False,  True,  True, False, False, False,  True,  True,  True, False,\n",
       "         False, False,  True,  True,  True, False, False, False, False, False,\n",
       "          True, False,  True,  True, False, False,  True,  True, False,  True,\n",
       "         False, False, False, False,  True, False,  True,  True,  True, False,\n",
       "          True,  True, False, False, False, False,  True, False, False,  True,\n",
       "         False, False, False, False,  True, False,  True, False,  True, False,\n",
       "          True, False,  True, False,  True,  True, False, False, False, False,\n",
       "          True, False, False,  True, False, False,  True, False, False, False,\n",
       "         False, False,  True, False, False, False,  True,  True, False, False,\n",
       "         False, False, False,  True, False, False,  True, False, False,  True,\n",
       "         False,  True, False, False, False, False, False, False,  True,  True,\n",
       "          True,  True,  True, False, False, False,  True, False, False, False,\n",
       "          True, False,  True, False, False, False, False,  True, False,  True,\n",
       "          True, False, False, False, False,  True, False,  True,  True,  True,\n",
       "         False, False, False, False,  True, False, False,  True, False, False,\n",
       "         False,  True,  True,  True,  True,  True,  True, False, False,  True,\n",
       "          True, False, False, False, False, False, False,  True,  True,  True,\n",
       "          True,  True,  True, False,  True,  True, False,  True,  True, False,\n",
       "         False, False,  True, False,  True, False, False,  True, False, False,\n",
       "         False]),\n",
       " array([[[1, 1, 1, 1],\n",
       "         [1, 1, 0, 0],\n",
       "         [1, 1, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[1, 1, 0, 0],\n",
       "         [1, 1, 1, 1],\n",
       "         [1, 1, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[1, 1, 0, 0],\n",
       "         [1, 1, 0, 0],\n",
       "         [1, 1, 1, 1],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        ...,\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [1, 1, 1, 1],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [1, 1, 1, 1],\n",
       "         [0, 0, 0, 0]],\n",
       " \n",
       "        [[0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         ...,\n",
       "         [0, 0, 0, 0],\n",
       "         [0, 0, 0, 0],\n",
       "         [1, 1, 1, 1]]]))"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_leskovec_dataset(multiplex_folder_path, size_x = 5):\n",
    "    les_data_folder = os.path.join(multiplex_folder_path, \"Leskovec-Ng Dataset\" )\n",
    "    edges_file_path = os.path.join(les_data_folder,\"Leskovec-Ng.multilayer.edges\")\n",
    "    labels = np.loadtxt(os.path.join(les_data_folder,'Leskovec-Ng.multilayer.labels')).astype(np.int32)\n",
    "    \n",
    "    data = np.loadtxt(fname=edges_file_path).astype(np.int32)\n",
    "    layers = [0, 1, 2, 3]\n",
    "    graphs = []\n",
    "    adj_mats = []\n",
    "    sum_ = 0\n",
    "    edges_df = pd.read_csv(edges_file_path, sep = \" \", header = None,  names = [\"layerId\", \"src\", \"dst\"],dtype=int)\n",
    "    print(edges_df['src'].min())\n",
    "    \n",
    "    for layer in layers : \n",
    "        df = edges_df[edges_df['layerId'] == layer]\n",
    "        G= nx.from_pandas_edgelist(df, source='src', target='dst',create_using = nx.DiGraph)\n",
    "        graphs.append(G)\n",
    "        adj_mat = nx.adjacency_matrix(G).todense()\n",
    "        \n",
    "        adj_mats.append(np.array(adj_mat,dtype=int))\n",
    "        \n",
    "        sum_ += adj_mat.sum()\n",
    "        print(\"# edges in layer {} are {}\".format( layer, adj_mat.sum()))\n",
    "    print(\"# edges are {}\".format( sum_))\n",
    "    \n",
    "    n = max(edges_df[\"src\"].max(), edges_df[\"dst\"].max())  + 1\n",
    "    print(\"# nodes are {}\".format( n ))\n",
    "    train_mask, test_mask = generate_train_test_mask(n)\n",
    "    random_X = np.random.normal(size = [n, size_x])\n",
    "    final_random_X = np.stack([random_X]* len(layers),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    \n",
    "    \n",
    "    return graphs, final_random_X, torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), adj\n",
    "    \n",
    "\n",
    "get_les_dataset(\"/home/keshav/courses/master_thesis/multiplex_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io\n",
    "import os\n",
    "def process_adj_mat(A):\n",
    "    A[A>0] = 1\n",
    "    return A.astype(int)\n",
    "\n",
    "def get_leskovec_true_dataset(multiplex_folder_path, size_x = 5):\n",
    "    data_folder = os.path.join(multiplex_folder_path, \"Leskovec-Ng Dataset\" )\n",
    "    file_names = [\"LN_2000_2004.mat\", \"LN_2005_2009.mat\" , \"LN_2010_2014.mat\"]\n",
    "    adj_mats = []\n",
    "    G = []\n",
    "    for i, file  in enumerate(file_names):\n",
    "        \n",
    "        mat1 = scipy.io.loadmat( os.path.join(data_folder, file))\n",
    "        \n",
    "        adj = process_adj_mat(mat1[\"A{}\".format(i+2)])\n",
    "        adj_mats.append(adj)\n",
    "        G.append(nx.convert_matrix.from_numpy_array(adj, create_using = nx.DiGraph))\n",
    "    labels_mat = scipy.io.loadmat( os.path.join(data_folder, \"LN_true.mat\"))\n",
    "    labels= np.array(labels_mat[\"s_LNG\"].flatten(), dtype = int)\n",
    "    n = adj_mats[0].shape[0]\n",
    "    train_mask, test_mask = generate_train_test_mask(n, args.train_fraction)\n",
    "    random_X = np.random.normal(size = [n, size_x])\n",
    "    final_random_X = np.stack([random_X]* len(file_names),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    \n",
    "    return G, final_random_X, torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), adj\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nx' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-570e5704467d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_leskovec_true_dataset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"/home/keshav/courses/master_thesis/multiplex_datasets\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-0ce501b51c7b>\u001b[0m in \u001b[0;36mget_leskovec_true_dataset\u001b[0;34m(multiplex_folder_path, size_x)\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0madj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprocess_adj_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmat1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"A{}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0madj_mats\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mG\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_matrix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_using\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDiGraph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0mlabels_mat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscipy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_folder\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"LN_true.mat\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mlabels\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels_mat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"s_LNG\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'nx' is not defined"
     ]
    }
   ],
   "source": [
    "get_leskovec_true_dataset(\"/home/keshav/courses/master_thesis/multiplex_datasets\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(\"/home/keshav/courses/master_thesis/PM/Datasets/WikipediaArticles.mat\")\n",
    "mat1 = scipy.io.loadmat( data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., ..., 0., 0., 0.],\n",
       "       [4., 1., 0., ..., 0., 0., 0.],\n",
       "       [1., 0., 0., ..., 0., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.],\n",
       "       [0., 0., 0., ..., 0., 0., 0.]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(mat1['data'][0,3].todense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<KeysViewHDF5 ['#refs#', 'W', 'data', 'truelabel']>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'_id': <h5py.h5g.GroupID at 0x7f758a297f68>}"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vars(f['W'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keshav/miniconda3/envs/graphEnv/lib/python3.6/site-packages/ipykernel_launcher.py:1: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(187, 1703)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[f['data'][0][0]].value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'int' object has no attribute 'encode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-2d2ae4011763>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'#refs#'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mh5py/_objects.pyx\u001b[0m in \u001b[0;36mh5py._objects.with_phil.wrapper\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/graphEnv/lib/python3.6/site-packages/h5py/_hl/group.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    262\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid HDF5 object reference\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0moid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5o\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_e\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlapl\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lapl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m         \u001b[0motype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5i\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/graphEnv/lib/python3.6/site-packages/h5py/_hl/base.py\u001b[0m in \u001b[0;36m_e\u001b[0;34m(self, name, lcpl)\u001b[0m\n\u001b[1;32m    135\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 137\u001b[0;31m                 \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ascii'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    138\u001b[0m                 \u001b[0mcoding\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mh5t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mCSET_ASCII\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    139\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mUnicodeEncodeError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'int' object has no attribute 'encode'"
     ]
    }
   ],
   "source": [
    "f[f['#refs#'][0][0]].value.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 187)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f[f['truelabel'][0][0]].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_W = f['W']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(578,)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_W['data'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(578,)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_W['ir'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(188,)"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f_W['jc'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_nx_adj_lap():\n",
    "    nx_g = data[0].to_networkx()\n",
    "    adj = np.array(nx.convert_matrix.to_numpy_matrix(nx_g))\n",
    "    adj_list = [adj]\n",
    "    graphs_list = [nx_g]\n",
    "    Ls = [sgwt_raw_laplacian(adj)]\n",
    "    features = torch.tensor(PCA(n_components=args.size_x).fit_transform(g.ndata['feat'].numpy()),dtype=torch.float).to(args.device)\n",
    "    features_list = [features]\n",
    "    if(args.create_similarity_layer):\n",
    "        adj_2 = np.array(kneighbors_graph(g.ndata['feat'].numpy(),n_neighbors = args.num_similarity_neighbors, metric = \"cosine\",include_self = True).todense())\n",
    "        nx_g2 = nx.convert_matrix.from_numpy_array(adj_2, create_using = nx.DiGraph)\n",
    "        adj_list.append(adj_2)\n",
    "        graphs_list.append(nx_g2)\n",
    "        features_list.append(features)\n",
    "        Ls.append(sgwt_raw_laplacian(adj_2))\n",
    "\n",
    "\n",
    "    adj_final = np.stack(adj_list,axis = 2)\n",
    "    L = np.stack(Ls, axis = 2)\n",
    "    features = torch.stack(features_list, axis = 2 )\n",
    "    process_adj_mat(adj_final, args)\n",
    "    args.update(graph_obj =graphs_list)\n",
    "    args.update(laplacian=L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ml_clustering_mat_dataset(args):\n",
    "    data_folder = args.ml_cluster_mat_folder\n",
    "    mat_file_path = os.path.join(data_folder, \"{}.mat\".format(args.dataset))\n",
    "    adj, feats , labels = mat_file_load_all(mat_file_path)\n",
    "    train_mask, test_mask = generate_train_test_mask(n, args.train_fraction)\n",
    "    nx_g = nx.convert_matrix.from_numpy_array(adj, create_using = nx.DiGraph)\n",
    "    nx_list = [nx_g]\n",
    "    adj_list = [adj]\n",
    "    Ls = [sgwt_raw_laplacian(adj)]\n",
    "    if args.size_x < feats.shape[1] :\n",
    "        feats = torch.tensor(PCA(n_components=args.size_x).fit_transform(feats),dtype=torch.float).to(args.device)\n",
    "    \n",
    "    features_list = [feats]\n",
    "    if(args.create_similarity_layer):\n",
    "        adj_2 = np.array(kneighbors_graph(feats ,n_neighbors = args.num_similarity_neighbors, metric = \"cosine\",include_self = True).todense())\n",
    "        nx_g2 = nx.convert_matrix.from_numpy_array(adj_2, create_using = nx.DiGraph)\n",
    "        adj_list.append(adj_2)\n",
    "        graphs_list.append(nx_g2)\n",
    "        features_list.append(feats)\n",
    "        Ls.append(sgwt_raw_laplacian(adj_2))\n",
    "    \n",
    "    adj_final = np.stack(adj_list,axis = 2)\n",
    "    L = np.stack(Ls, axis = 2)\n",
    "    features = torch.stack(features_list, axis = 2 ).to(args.device)\n",
    "\n",
    "    \n",
    "    return nx_list, features , torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), L ,adj\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<KeysViewHDF5 ['#refs#', 'data', 'truelabel']>\n",
      "['#refs#']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/keshav/miniconda3/envs/graphEnv/lib/python3.6/site-packages/ipykernel_launcher.py:5: H5pyDeprecationWarning: The default file mode will change to 'r' (read-only) in h5py 3.0. To suppress this warning, pass the mode you need to h5py.File(), or set the global default h5.get_config().default_file_mode, or set the environment variable H5PY_DEFAULT_READONLY=1. Available modes are: 'r', 'r+', 'w', 'w-'/'x', 'a'. See the docs for details.\n",
      "  \"\"\"\n",
      "/home/keshav/miniconda3/envs/graphEnv/lib/python3.6/site-packages/ipykernel_launcher.py:10: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "/home/keshav/miniconda3/envs/graphEnv/lib/python3.6/site-packages/ipykernel_launcher.py:11: H5pyDeprecationWarning: dataset.value has been deprecated. Use dataset[()] instead.\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "((2000, 216), array([0., 0., 0., ..., 9., 9., 9.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy import sparse\n",
    "import numpy as np\n",
    "import h5py\n",
    "def mat_file_load_all(fname) :\n",
    "    f = h5py.File(fname)\n",
    "    print(f.keys())\n",
    "    #f_W = f['W']\n",
    "    #M = np.array(sparse.csc_matrix( (f_W['data'], f_W['ir'], f_W['jc']) ).todense())\n",
    "    print(['#refs#'])\n",
    "    features = f[f['data'][0][0]].value\n",
    "    labels = f[f[\"truelabel\"][0][0]].value.squeeze()\n",
    "    f.close()\n",
    "    \n",
    "    return features.shape, labels\n",
    "\n",
    "mat_file_load_all(\"/home/keshav/courses/master_thesis/PM/Datasets/UCI_mfeat.mat\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mat1['data'].squeeze().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_ml_clustering_scipymat_dataset(args):\n",
    "    data_folder = args.ml_cluster_mat_folder\n",
    "    mat_file_path = os.path.join(data_folder, \"{}.mat\".format(args.dataset))\n",
    "    mat1 = scipy.io.loadmat( mat_file_path)\n",
    "    num_layers = mat1['data'].shape[1]\n",
    "    print(\"# num layers {}\".format(num_layers))\n",
    "    labels = mat1['truelabel'][0,0]\n",
    "    print(\"# num nodes {}\".format( len(labels)))\n",
    "    n = len(labels)\n",
    "    feats_list = []\n",
    "    nx_list = []\n",
    "    adj_list = []\n",
    "    Ls = []\n",
    "    for i in num_layers:\n",
    "        print(\"# current layer {}\".format(i))\n",
    "        feats = mat1['data'][0,i].T\n",
    "        \n",
    "        print(feats.shape)\n",
    "        adj = np.array(kneighbors_graph(feats ,n_neighbors = args.num_similarity_neighbors, metric = \"cosine\",include_self = True).todense())\n",
    "        if(args.scale_features):\n",
    "            feats_scaled = sklearn.preprocessing.scale(feats)\n",
    "        else:\n",
    "            feats_scaled = feats\n",
    "        if args.size_x < feats.shape[1] :\n",
    "            features = torch.tensor(PCA(n_components=args.size_x).fit_transform(feats_scaled),dtype=torch.float).to(args.device)\n",
    "        else:\n",
    "            features = torch.tensor(feats_scaled,dtype=torch.float).to(args.device)\n",
    "        feats_list.append(features)\n",
    "        nx_list.append(nx.convert_matrix.from_numpy_array(adj, create_using = nx.DiGraph))\n",
    "        adj_list.append(adj)\n",
    "        Ls.append(sgwt_raw_laplacian(adj))\n",
    "        \n",
    "    \n",
    "    train_mask, test_mask = generate_train_test_mask(n, args.train_fraction)\n",
    "    adj_final = np.stack(adj_list,axis = 2)\n",
    "    L = np.stack(Ls, axis = 2)\n",
    "    features = torch.stack(features_list, axis = 2 ).to(args.device)\n",
    "    \n",
    "\n",
    "    \n",
    "    return nx_list, features , torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask), L ,adj_final\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n        adj = process_adj_mat(mat1[\"A{}\".format(i+1)])\\n        Ls.append(sgwt_raw_laplacian(adj))\\n        adj_mats.append(adj)\\n        idx_nonzeros = np.nonzero(adj)\\n        for (src,dst) in zip(idx_nonzeros[0],idx_nonzeros[1]):\\n            edges.append([i,src,dst])\\n        print(\"# edges in layer {} are {}\".format( i + 1, adj.sum()))\\n        sum_ += adj.sum()\\n        G.append(nx.convert_matrix.from_numpy_array(adj, create_using = nx.DiGraph))\\n    labels_mat = scipy.io.loadmat( os.path.join(data_folder, \"LN_true.mat\"))\\n    labels= np.array(labels_mat[\"s_LNG\"].flatten(), dtype = int) - 1\\n    print(\"# edges are {}\".format( sum_))\\n    n = adj_mats[0].shape[0]\\n    L = np.stack(Ls,axis = 2)\\n    train_mask, test_mask = generate_train_test_mask(n, args.train_fraction)\\n    random_X = np.random.normal(size = [n, size_x])\\n    final_random_X = np.stack([random_X]* len(file_names),axis = 2)\\n    adj = np.stack(adj_mats, axis = 2)\\n    final_random_X = torch.from_numpy(final_random_X).float()\\n    print(\"# nodes are {}\".format( n ))\\n    print(\"# train samples are {}\".format(train_mask.sum()))\\n    print(\"# test samples are {}\".format(test_mask.sum()))\\n    if(args.save_input_list):\\n        edges_np = np.array(edges,dtype=int)    \\n        np.savetxt(os.path.join(data_folder,\"leskovec_multiple_edges.txt\"),edges_np,fmt=\\'%i\\')\\n        np.savetxt(os.path.join(data_folder,\"leskovec_labels.txt\"),labels,fmt=\\'%i\\')\\n        print(\"saved to {}\".format(data_folder))\\n    return G, final_random_X, torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask),L, adj\\n    '"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_uci_true_dataset(args):\n",
    "    multiplex_folder_path= args.multiplex_folder_path\n",
    "    #size_x = args.size_x\n",
    "    data_folder = os.path.join(multiplex_folder_path, \"UCI\",  \"mfeat\")\n",
    "    file_names = [\"mfeat-fac\" , \"mfeat-fou\", \"mfeat-kar\", \"mfeat-mor\" , \"mfeat-pix\" , \"mfeat-zer\"]\n",
    "    adj_mats = []\n",
    "    edges = []\n",
    "    G = []\n",
    "    Ls = []\n",
    "    sum_ = 0\n",
    "    labels = [[i] * 200 for i in range ( 10 )]\n",
    "    np_labels = np.array(labels).flatten()\n",
    "    feats_list = []\n",
    "    nx_list = []\n",
    "    adj_list = []\n",
    "    Ls = []\n",
    "    for i, file  in enumerate(file_names):\n",
    "        print(os.path.join(data_folder, file))\n",
    "        print(\"# current layer {}\".format(i))\n",
    "        with open(os.path.join(data_folder, file),'r') as f:\n",
    "            mat = f.readlines()\n",
    "        #print(mat)\n",
    "        mat_2d = [ l.split() for l in mat]\n",
    "        np_ary = np.array(mat_2d, dtype = np.float)\n",
    "        feats = np_ary\n",
    "        adj = np.array(kneighbors_graph(feats ,n_neighbors = args.num_similarity_neighbors, metric = \"cosine\",include_self = True).todense())\n",
    "        if(args.scale_features):\n",
    "            feats_scaled = sklearn.preprocessing.scale(feats)\n",
    "        else:\n",
    "            feats_scaled = feats\n",
    "        if args.size_x < feats.shape[1] :\n",
    "            features = torch.tensor(PCA(n_components=args.size_x).fit_transform(feats_scaled),dtype=torch.float).to(args.device)\n",
    "        else:\n",
    "            features = torch.tensor(feats_scaled,dtype=torch.float).to(args.device)\n",
    "        feats_list.append(features)\n",
    "        nx_list.append(nx.convert_matrix.from_numpy_array(adj, create_using = nx.DiGraph))\n",
    "        adj_list.append(adj)\n",
    "        Ls.append(sgwt_raw_laplacian(adj))\n",
    "        \n",
    "    \n",
    "    train_mask, test_mask = generate_train_test_mask(n, args.train_fraction)\n",
    "    adj_final = np.stack(adj_list,axis = 2)\n",
    "    L = np.stack(Ls, axis = 2)\n",
    "    features = torch.stack(features_list, axis = 2 ).to(args.device)\n",
    "    \n",
    "        #mat1 = pd.read_csv( os.path.join(data_folder, file), sep = \" \", header = None, ).to_numpy()\n",
    "        #print(mat1.shape)\n",
    "        \n",
    "    return None\n",
    "'''\n",
    "        adj = process_adj_mat(mat1[\"A{}\".format(i+1)])\n",
    "        Ls.append(sgwt_raw_laplacian(adj))\n",
    "        adj_mats.append(adj)\n",
    "        idx_nonzeros = np.nonzero(adj)\n",
    "        for (src,dst) in zip(idx_nonzeros[0],idx_nonzeros[1]):\n",
    "            edges.append([i,src,dst])\n",
    "        print(\"# edges in layer {} are {}\".format( i + 1, adj.sum()))\n",
    "        sum_ += adj.sum()\n",
    "        G.append(nx.convert_matrix.from_numpy_array(adj, create_using = nx.DiGraph))\n",
    "    labels_mat = scipy.io.loadmat( os.path.join(data_folder, \"LN_true.mat\"))\n",
    "    labels= np.array(labels_mat[\"s_LNG\"].flatten(), dtype = int) - 1\n",
    "    print(\"# edges are {}\".format( sum_))\n",
    "    n = adj_mats[0].shape[0]\n",
    "    L = np.stack(Ls,axis = 2)\n",
    "    train_mask, test_mask = generate_train_test_mask(n, args.train_fraction)\n",
    "    random_X = np.random.normal(size = [n, size_x])\n",
    "    final_random_X = np.stack([random_X]* len(file_names),axis = 2)\n",
    "    adj = np.stack(adj_mats, axis = 2)\n",
    "    final_random_X = torch.from_numpy(final_random_X).float()\n",
    "    print(\"# nodes are {}\".format( n ))\n",
    "    print(\"# train samples are {}\".format(train_mask.sum()))\n",
    "    print(\"# test samples are {}\".format(test_mask.sum()))\n",
    "    if(args.save_input_list):\n",
    "        edges_np = np.array(edges,dtype=int)    \n",
    "        np.savetxt(os.path.join(data_folder,\"leskovec_multiple_edges.txt\"),edges_np,fmt='%i')\n",
    "        np.savetxt(os.path.join(data_folder,\"leskovec_labels.txt\"),labels,fmt='%i')\n",
    "        print(\"saved to {}\".format(data_folder))\n",
    "    return G, final_random_X, torch.from_numpy(labels),  torch.from_numpy(train_mask), torch.from_numpy(test_mask), torch.from_numpy(test_mask),L, adj\n",
    "    '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "from U2GNN_pytorch import util\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "args = {}\n",
    "args['multiplex_folder_path'] = \"/home/keshav/courses/master_thesis/multiplex_datasets\"\n",
    "args = util.Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2000,)\n",
      "/home/keshav/courses/master_thesis/multiplex_datasets/UCI/mfeat/mfeat-fac\n",
      "(2000, 216)\n",
      "/home/keshav/courses/master_thesis/multiplex_datasets/UCI/mfeat/mfeat-fou\n",
      "(2000, 76)\n",
      "/home/keshav/courses/master_thesis/multiplex_datasets/UCI/mfeat/mfeat-kar\n",
      "(2000, 64)\n",
      "/home/keshav/courses/master_thesis/multiplex_datasets/UCI/mfeat/mfeat-mor\n",
      "(2000, 6)\n",
      "/home/keshav/courses/master_thesis/multiplex_datasets/UCI/mfeat/mfeat-pix\n",
      "(2000, 240)\n",
      "/home/keshav/courses/master_thesis/multiplex_datasets/UCI/mfeat/mfeat-zer\n",
      "(2000, 47)\n"
     ]
    }
   ],
   "source": [
    "get_uci_true_dataset(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
